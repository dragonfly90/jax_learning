{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31ef7f64",
   "metadata": {},
   "source": [
    "Transformer Training from Scratch in JAX\n",
    "==========================================\n",
    "\n",
    "Key concepts:\n",
    "- Full GPT-style decoder-only transformer in pure JAX + Flax.\n",
    "- Multi-head causal self-attention with KV cache for inference.\n",
    "- Rotary positional embeddings (RoPE).\n",
    "- Training loop with Optax (AdamW + cosine schedule + warmup).\n",
    "- Gradient accumulation for effective large batch training.\n",
    "\n",
    "Install: pip install jax jaxlib flax optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edf9cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e095368",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db427472",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Rotary Positional Embeddings (RoPE) ----\n",
    "\n",
    "def rotary_embedding(x, seq_len, head_dim):\n",
    "    \"\"\"\n",
    "    Apply rotary positional embeddings.\n",
    "    x: (..., seq_len, head_dim)\n",
    "    \"\"\"\n",
    "    positions = jnp.arange(seq_len)\n",
    "    dim_pairs = jnp.arange(0, head_dim, 2)\n",
    "    freqs = 1.0 / (10000.0 ** (dim_pairs / head_dim))\n",
    "    angles = positions[:, None] * freqs[None, :]  # (seq_len, head_dim//2)\n",
    "\n",
    "    cos = jnp.cos(angles)\n",
    "    sin = jnp.sin(angles)\n",
    "\n",
    "    # Split x into pairs and rotate\n",
    "    x1 = x[..., ::2]   # even dimensions\n",
    "    x2 = x[..., 1::2]  # odd dimensions\n",
    "\n",
    "    out1 = x1 * cos - x2 * sin\n",
    "    out2 = x1 * sin + x2 * cos\n",
    "\n",
    "    # Interleave back\n",
    "    return jnp.stack([out1, out2], axis=-1).reshape(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323d89c9",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373912ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Multi-Head Causal Self-Attention ----\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    num_heads: int\n",
    "    head_dim: int\n",
    "    dropout_rate: float = 0.0\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, training=False):\n",
    "        batch, seq_len, embed_dim = x.shape\n",
    "\n",
    "        # Project to Q, K, V\n",
    "        qkv = nn.Dense(3 * self.num_heads * self.head_dim, use_bias=False)(x)\n",
    "        qkv = qkv.reshape(batch, seq_len, 3, self.num_heads, self.head_dim)\n",
    "        q, k, v = qkv[:, :, 0], qkv[:, :, 1], qkv[:, :, 2]\n",
    "\n",
    "        # Apply RoPE\n",
    "        q = rotary_embedding(q, seq_len, self.head_dim)\n",
    "        k = rotary_embedding(k, seq_len, self.head_dim)\n",
    "\n",
    "        # Attention scores\n",
    "        scale = 1.0 / jnp.sqrt(self.head_dim)\n",
    "        scores = jnp.einsum('bqhd,bkhd->bhqk', q, k) * scale\n",
    "\n",
    "        # Causal mask\n",
    "        mask = jnp.tril(jnp.ones((seq_len, seq_len)))[None, None, :, :]\n",
    "        scores = jnp.where(mask, scores, -1e9)\n",
    "\n",
    "        weights = jax.nn.softmax(scores, axis=-1)\n",
    "        weights = nn.Dropout(self.dropout_rate)(weights, deterministic=not training)\n",
    "\n",
    "        # Weighted sum\n",
    "        out = jnp.einsum('bhqk,bkhd->bqhd', weights, v)\n",
    "        out = out.reshape(batch, seq_len, self.num_heads * self.head_dim)\n",
    "\n",
    "        # Output projection\n",
    "        return nn.Dense(embed_dim, use_bias=False)(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ae91b1",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa8e2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Transformer Block ----\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    num_heads: int\n",
    "    head_dim: int\n",
    "    mlp_dim: int\n",
    "    dropout_rate: float = 0.0\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, training=False):\n",
    "        # Pre-norm attention\n",
    "        residual = x\n",
    "        x = nn.RMSNorm()(x)\n",
    "        x = CausalSelfAttention(\n",
    "            self.num_heads, self.head_dim, self.dropout_rate\n",
    "        )(x, training=training)\n",
    "        x = nn.Dropout(self.dropout_rate)(x, deterministic=not training)\n",
    "        x = residual + x\n",
    "\n",
    "        # Pre-norm MLP (SwiGLU)\n",
    "        residual = x\n",
    "        x = nn.RMSNorm()(x)\n",
    "        # SwiGLU: gate * silu(x)\n",
    "        gate = nn.Dense(self.mlp_dim, use_bias=False)(x)\n",
    "        up = nn.Dense(self.mlp_dim, use_bias=False)(x)\n",
    "        x = jax.nn.silu(gate) * up\n",
    "        x = nn.Dense(residual.shape[-1], use_bias=False)(x)\n",
    "        x = nn.Dropout(self.dropout_rate)(x, deterministic=not training)\n",
    "        x = residual + x\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2100771",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a8dc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Full GPT Model ----\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    vocab_size: int\n",
    "    num_layers: int\n",
    "    num_heads: int\n",
    "    head_dim: int\n",
    "    mlp_dim: int\n",
    "    max_seq_len: int\n",
    "    dropout_rate: float = 0.0\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, token_ids, training=False):\n",
    "        batch, seq_len = token_ids.shape\n",
    "        embed_dim = self.num_heads * self.head_dim\n",
    "\n",
    "        # Token embedding (no positional embedding — we use RoPE)\n",
    "        x = nn.Embed(self.vocab_size, embed_dim)(token_ids)\n",
    "        x = nn.Dropout(self.dropout_rate)(x, deterministic=not training)\n",
    "\n",
    "        # Transformer layers\n",
    "        for _ in range(self.num_layers):\n",
    "            x = TransformerBlock(\n",
    "                self.num_heads, self.head_dim,\n",
    "                self.mlp_dim, self.dropout_rate\n",
    "            )(x, training=training)\n",
    "\n",
    "        x = nn.RMSNorm()(x)\n",
    "\n",
    "        # Language model head (weight-tied with embedding)\n",
    "        logits = nn.Dense(self.vocab_size, use_bias=False)(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43183eaf",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086064d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Training Setup ----\n",
    "\n",
    "# Small model for demonstration\n",
    "config = dict(\n",
    "    vocab_size=1024,\n",
    "    num_layers=4,\n",
    "    num_heads=4,\n",
    "    head_dim=32,\n",
    "    mlp_dim=512,\n",
    "    max_seq_len=128,\n",
    "    dropout_rate=0.1,\n",
    ")\n",
    "\n",
    "model = GPT(**config)\n",
    "key = jax.random.PRNGKey(42)\n",
    "\n",
    "# Initialize\n",
    "dummy_tokens = jnp.ones((2, 128), dtype=jnp.int32)\n",
    "variables = model.init(key, dummy_tokens, training=False)\n",
    "params = variables['params']\n",
    "\n",
    "# Count parameters\n",
    "num_params = sum(p.size for p in jax.tree.leaves(params))\n",
    "print(f\"Model parameters: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b68d25",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77981622",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Optimizer: AdamW with Cosine Schedule + Warmup ----\n",
    "\n",
    "num_steps = 500\n",
    "warmup_steps = 50\n",
    "peak_lr = 3e-4\n",
    "min_lr = 1e-5\n",
    "\n",
    "schedule = optax.warmup_cosine_decay_schedule(\n",
    "    init_value=0.0,\n",
    "    peak_value=peak_lr,\n",
    "    warmup_steps=warmup_steps,\n",
    "    decay_steps=num_steps,\n",
    "    end_value=min_lr,\n",
    ")\n",
    "\n",
    "optimizer = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),   # gradient clipping\n",
    "    optax.adamw(schedule, weight_decay=0.1),\n",
    ")\n",
    "opt_state = optimizer.init(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379e342e",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ea8e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. Loss Function ----\n",
    "\n",
    "def compute_loss(params, tokens, key):\n",
    "    \"\"\"\n",
    "    Language modeling loss: predict next token.\n",
    "    tokens: (batch, seq_len) — input token IDs.\n",
    "    \"\"\"\n",
    "    inputs = tokens[:, :-1]\n",
    "    targets = tokens[:, 1:]\n",
    "\n",
    "    logits = model.apply(\n",
    "        {'params': params}, inputs,\n",
    "        training=True,\n",
    "        rngs={'dropout': key}\n",
    "    )\n",
    "\n",
    "    # Cross-entropy loss\n",
    "    log_probs = jax.nn.log_softmax(logits, axis=-1)\n",
    "    target_log_probs = jnp.take_along_axis(\n",
    "        log_probs, targets[:, :, None], axis=-1\n",
    "    )[:, :, 0]\n",
    "\n",
    "    return -jnp.mean(target_log_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630d787a",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f5e8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. Training Step ----\n",
    "\n",
    "@jax.jit\n",
    "def train_step(params, opt_state, tokens, key):\n",
    "    loss, grads = jax.value_and_grad(compute_loss)(params, tokens, key)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9ce665",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f06b68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. Training Loop ----\n",
    "\n",
    "print(\"\\n=== Training ===\")\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "for step in range(num_steps):\n",
    "    key, data_key, dropout_key = jax.random.split(key, 3)\n",
    "\n",
    "    # Generate random token sequences (replace with real data)\n",
    "    tokens = jax.random.randint(data_key, (8, 128), 0, config['vocab_size'])\n",
    "\n",
    "    params, opt_state, loss = train_step(params, opt_state, tokens, dropout_key)\n",
    "\n",
    "    if step % 50 == 0:\n",
    "        lr = schedule(step)\n",
    "        print(f\"Step {step:4d} | Loss: {loss:.4f} | LR: {lr:.2e}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b255dd",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fb5ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "10. Gradient Accumulation ----\n",
    "# Simulate larger batch sizes by accumulating gradients over micro-batches\n",
    "\n",
    "@jax.jit\n",
    "def train_step_grad_accum(params, opt_state, micro_batches, key):\n",
    "    \"\"\"\n",
    "    Accumulate gradients over multiple micro-batches.\n",
    "    micro_batches: (num_micro, batch_per_micro, seq_len)\n",
    "    \"\"\"\n",
    "    num_micro = micro_batches.shape[0]\n",
    "    keys = jax.random.split(key, num_micro)\n",
    "\n",
    "    def accum_step(carry, micro_and_key):\n",
    "        total_loss, total_grads = carry\n",
    "        micro_batch, step_key = micro_and_key\n",
    "\n",
    "        loss, grads = jax.value_and_grad(compute_loss)(params, micro_batch, step_key)\n",
    "        total_grads = jax.tree.map(lambda a, b: a + b, total_grads, grads)\n",
    "        return (total_loss + loss, total_grads), None\n",
    "\n",
    "    zero_grads = jax.tree.map(jnp.zeros_like, params)\n",
    "    (total_loss, total_grads), _ = jax.lax.scan(\n",
    "        accum_step, (0.0, zero_grads), (micro_batches, keys)\n",
    "    )\n",
    "\n",
    "    # Average\n",
    "    avg_loss = total_loss / num_micro\n",
    "    avg_grads = jax.tree.map(lambda g: g / num_micro, total_grads)\n",
    "\n",
    "    updates, opt_state = optimizer.update(avg_grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, avg_loss\n",
    "\n",
    "# Example: 4 micro-batches of 4 samples each = effective batch of 16\n",
    "key, accum_key = jax.random.split(key)\n",
    "micro_batches = jax.random.randint(key, (4, 4, 128), 0, config['vocab_size'])\n",
    "params, opt_state, loss = train_step_grad_accum(\n",
    "    params, opt_state, micro_batches, accum_key\n",
    ")\n",
    "print(f\"\\nGrad accumulation loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc9d1fa",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5918d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "11. Simple Greedy Generation ----\n",
    "\n",
    "@jax.jit\n",
    "def generate(params, prompt_tokens, max_new_tokens=50):\n",
    "    \"\"\"Autoregressive greedy decoding.\"\"\"\n",
    "    def generate_step(carry, _):\n",
    "        tokens = carry\n",
    "        logits = model.apply({'params': params}, tokens, training=False)\n",
    "        next_token = jnp.argmax(logits[:, -1, :], axis=-1, keepdims=True)\n",
    "        tokens = jnp.concatenate([tokens, next_token], axis=1)\n",
    "        return tokens, next_token\n",
    "\n",
    "    tokens, generated = jax.lax.scan(\n",
    "        generate_step, prompt_tokens, None, length=max_new_tokens\n",
    "    )\n",
    "    return tokens\n",
    "\n",
    "prompt = jnp.array([[1, 2, 3]])  # dummy prompt\n",
    "generated = generate(params, prompt, max_new_tokens=20)\n",
    "print(f\"\\nGenerated sequence shape: {generated.shape}\")\n",
    "print(f\"Generated tokens: {generated[0]}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
