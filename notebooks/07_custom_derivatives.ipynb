{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7434afb6",
   "metadata": {},
   "source": [
    "Custom Derivatives and Advanced Autodiff\n",
    "==========================================\n",
    "\n",
    "Key concepts:\n",
    "- jax.custom_jvp: Define custom forward-mode derivatives.\n",
    "- jax.custom_vjp: Define custom reverse-mode derivatives (backprop).\n",
    "- stop_gradient: Prevent gradients from flowing through a computation.\n",
    "- Useful for: numerical stability, straight-through estimators,\n",
    "  custom backprop rules, and interfacing with non-JAX code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49da5008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c300c0",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8a32c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. custom_jvp: Custom Forward-Mode Derivative ----\n",
    "# Use when you want to override how JAX differentiates a function\n",
    "\n",
    "@jax.custom_jvp\n",
    "def safe_log(x):\n",
    "    \"\"\"log(x) that handles x=0 gracefully.\"\"\"\n",
    "    return jnp.log(x)\n",
    "\n",
    "@safe_log.defjvp\n",
    "def safe_log_jvp(primals, tangents):\n",
    "    \"\"\"Custom JVP: d/dx log(x) = 1/x, but clip for numerical stability.\"\"\"\n",
    "    x, = primals\n",
    "    x_dot, = tangents\n",
    "    primal_out = safe_log(x)\n",
    "    # Clip the gradient to avoid inf when x is near 0\n",
    "    tangent_out = x_dot / jnp.maximum(x, 1e-6)\n",
    "    return primal_out, tangent_out\n",
    "\n",
    "# Standard log would give inf gradient at x=0\n",
    "grad_safe = jax.grad(safe_log)(0.0001)\n",
    "print(f\"safe_log grad at 0.0001: {grad_safe:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d5494a",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7a14b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. custom_vjp: Custom Reverse-Mode Derivative ----\n",
    "# More common in deep learning (backpropagation)\n",
    "\n",
    "@jax.custom_vjp\n",
    "def straight_through_relu(x):\n",
    "    \"\"\"ReLU with straight-through estimator gradient.\"\"\"\n",
    "    return jnp.maximum(x, 0)\n",
    "\n",
    "def straight_through_relu_fwd(x):\n",
    "    return straight_through_relu(x), x  # save x as \"residual\" for backward\n",
    "\n",
    "def straight_through_relu_bwd(x, g):\n",
    "    \"\"\"Straight-through: pass gradient through regardless of sign.\"\"\"\n",
    "    # Normal ReLU would zero out gradient for x < 0\n",
    "    # Straight-through passes it through\n",
    "    return (g,)\n",
    "\n",
    "straight_through_relu.defvjp(straight_through_relu_fwd, straight_through_relu_bwd)\n",
    "\n",
    "# Compare gradients:\n",
    "normal_relu_grad = jax.grad(lambda x: jnp.sum(jax.nn.relu(x)))(jnp.array([-1.0, 0.5, -0.5, 1.0]))\n",
    "st_relu_grad = jax.grad(lambda x: jnp.sum(straight_through_relu(x)))(jnp.array([-1.0, 0.5, -0.5, 1.0]))\n",
    "print(f\"\\nNormal ReLU grad: {normal_relu_grad}\")\n",
    "print(f\"Straight-through grad: {st_relu_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0a8a8b",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941aec57",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. stop_gradient ----\n",
    "# Prevents gradients from flowing through a sub-expression\n",
    "\n",
    "def loss_with_target_net(params, target_params, x):\n",
    "    \"\"\"\n",
    "    DQN-style loss where target network gradients are stopped.\n",
    "    Only params receives gradients, not target_params.\n",
    "    \"\"\"\n",
    "    pred = jnp.dot(x, params)\n",
    "    # stop_gradient: treat target as a constant during backprop\n",
    "    target = jax.lax.stop_gradient(jnp.dot(x, target_params))\n",
    "    return jnp.mean((pred - target) ** 2)\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "params = jax.random.normal(key, (3,))\n",
    "target_params = jax.random.normal(key, (3,))\n",
    "x = jax.random.normal(key, (5, 3))\n",
    "\n",
    "grad_params = jax.grad(loss_with_target_net)(params, target_params, x)\n",
    "print(f\"\\nGradient w.r.t params: {grad_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4129dd8a",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761ebc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Gradient clipping via custom_vjp ----\n",
    "\n",
    "@jax.custom_vjp\n",
    "def clip_gradient(x):\n",
    "    return x\n",
    "\n",
    "def clip_gradient_fwd(x):\n",
    "    return x, ()\n",
    "\n",
    "def clip_gradient_bwd(_, g):\n",
    "    return (jnp.clip(g, -1.0, 1.0),)\n",
    "\n",
    "clip_gradient.defvjp(clip_gradient_fwd, clip_gradient_bwd)\n",
    "\n",
    "def unstable_fn(x):\n",
    "    return clip_gradient(x ** 3)\n",
    "\n",
    "# Without clipping, grad at x=100 would be 30000\n",
    "# With clipping, it's clamped to 1.0\n",
    "print(f\"\\nClipped gradient at x=100: {jax.grad(unstable_fn)(100.0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674c45c5",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1f5364",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Practical: Custom softmax with log-sum-exp trick ----\n",
    "\n",
    "@jax.custom_jvp\n",
    "def stable_softmax(x):\n",
    "    \"\"\"Numerically stable softmax.\"\"\"\n",
    "    x_max = jnp.max(x, axis=-1, keepdims=True)\n",
    "    exp_x = jnp.exp(x - x_max)\n",
    "    return exp_x / jnp.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "@stable_softmax.defjvp\n",
    "def stable_softmax_jvp(primals, tangents):\n",
    "    x, = primals\n",
    "    x_dot, = tangents\n",
    "    s = stable_softmax(x)\n",
    "    # Jacobian-vector product for softmax: s * (x_dot - <s, x_dot>)\n",
    "    tangent_out = s * (x_dot - jnp.sum(s * x_dot, axis=-1, keepdims=True))\n",
    "    return s, tangent_out\n",
    "\n",
    "logits = jnp.array([1.0, 2.0, 3.0, 100.0])  # extreme value\n",
    "probs = stable_softmax(logits)\n",
    "grad = jax.jacobian(stable_softmax)(logits)\n",
    "print(f\"\\nStable softmax: {probs}\")\n",
    "print(f\"Jacobian diagonal: {jnp.diag(grad)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8117283d",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0765fc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Checkpointing (recomputation to save memory) ----\n",
    "# jax.checkpoint recomputes forward pass during backward to save memory\n",
    "\n",
    "@jax.checkpoint\n",
    "def heavy_layer(x):\n",
    "    \"\"\"This layer's activations won't be stored â€” recomputed during backprop.\"\"\"\n",
    "    x = jnp.sin(x)\n",
    "    x = jnp.cos(x)\n",
    "    x = jnp.tanh(x)\n",
    "    return x\n",
    "\n",
    "def model(x):\n",
    "    for _ in range(10):\n",
    "        x = heavy_layer(x)\n",
    "    return jnp.sum(x)\n",
    "\n",
    "x = jax.random.normal(key, (100,))\n",
    "grad = jax.grad(model)(x)\n",
    "print(f\"\\nCheckpointed gradient shape: {grad.shape}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
