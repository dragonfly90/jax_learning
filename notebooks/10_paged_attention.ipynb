{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b33c2596",
   "metadata": {},
   "source": [
    "Paged Attention in JAX\n",
    "=======================\n",
    "\n",
    "Key concepts:\n",
    "- Paged Attention is used in LLM serving (vLLM) to manage KV cache memory\n",
    "  efficiently during autoregressive generation.\n",
    "- Instead of pre-allocating contiguous KV cache for max_seq_len, memory is\n",
    "  divided into fixed-size \"pages\" (blocks) allocated on demand.\n",
    "- A page table maps each sequence's logical positions to physical page indices.\n",
    "- This eliminates memory waste from padding and enables flexible memory sharing\n",
    "  (e.g., beam search candidates can share prefix pages via copy-on-write).\n",
    "\n",
    "Reference: \"Efficient Memory Management for Large Language Model Serving\n",
    "with PagedAttention\" (Kwon et al., 2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f2ed4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a97416a",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37dc4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Standard KV Cache (Wasteful) ----\n",
    "# Pre-allocates [batch, max_seq_len, heads, dim] — wastes memory on padding\n",
    "\n",
    "def standard_kv_cache_attention(q, k_cache, v_cache, seq_lens):\n",
    "    \"\"\"\n",
    "    q: (batch, 1, heads, dim) — current query token\n",
    "    k_cache, v_cache: (batch, max_seq_len, heads, dim)\n",
    "    seq_lens: (batch,) — actual sequence lengths\n",
    "    \"\"\"\n",
    "    batch, max_len, heads, dim = k_cache.shape\n",
    "    scale = 1.0 / jnp.sqrt(dim)\n",
    "\n",
    "    # (batch, heads, 1, max_len)\n",
    "    scores = jnp.einsum('bqhd,bkhd->bhqk', q, k_cache) * scale\n",
    "\n",
    "    # Mask out padding positions\n",
    "    positions = jnp.arange(max_len)[None, None, None, :]  # (1,1,1,max_len)\n",
    "    mask = positions < seq_lens[:, None, None, None]        # (batch,1,1,max_len)\n",
    "    scores = jnp.where(mask, scores, -1e9)\n",
    "\n",
    "    weights = jax.nn.softmax(scores, axis=-1)\n",
    "    return jnp.einsum('bhqk,bkhd->bqhd', weights, v_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2f8d8d",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3201c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Paged KV Cache Data Structure ----\n",
    "\n",
    "class PagedKVCache:\n",
    "    \"\"\"\n",
    "    Paged KV cache for efficient LLM serving.\n",
    "\n",
    "    Physical layout:\n",
    "      k_pages: (num_pages, page_size, num_heads, head_dim)\n",
    "      v_pages: (num_pages, page_size, num_heads, head_dim)\n",
    "\n",
    "    Logical mapping:\n",
    "      page_table: (batch, max_pages_per_seq) — maps logical page index\n",
    "                  to physical page index for each sequence.\n",
    "      seq_lens: (batch,) — actual token count per sequence.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_pages, page_size, num_heads, head_dim):\n",
    "        self.page_size = page_size\n",
    "        self.num_pages = num_pages\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "\n",
    "        # Physical page pool (shared across all sequences)\n",
    "        self.k_pages = jnp.zeros((num_pages, page_size, num_heads, head_dim))\n",
    "        self.v_pages = jnp.zeros((num_pages, page_size, num_heads, head_dim))\n",
    "\n",
    "        # Track which pages are free\n",
    "        self.free_pages = list(range(num_pages))\n",
    "\n",
    "    def allocate_page(self):\n",
    "        \"\"\"Get a free page index.\"\"\"\n",
    "        if not self.free_pages:\n",
    "            raise RuntimeError(\"Out of KV cache pages!\")\n",
    "        return self.free_pages.pop(0)\n",
    "\n",
    "    def free_page(self, page_idx):\n",
    "        \"\"\"Return a page to the free pool.\"\"\"\n",
    "        self.free_pages.append(page_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbc30f2",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62266400",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Paged Attention Kernel ----\n",
    "\n",
    "@partial(jax.jit, static_argnames=['page_size'])\n",
    "def paged_attention(\n",
    "    q,            # (batch, 1, num_heads, head_dim) — single query token\n",
    "    k_pages,      # (num_pages, page_size, num_heads, head_dim)\n",
    "    v_pages,      # (num_pages, page_size, num_heads, head_dim)\n",
    "    page_table,   # (batch, max_pages_per_seq) — physical page indices\n",
    "    seq_lens,     # (batch,) — actual sequence lengths\n",
    "    page_size=16,\n",
    "):\n",
    "    \"\"\"\n",
    "    Paged attention: look up KV from pages via the page table.\n",
    "\n",
    "    Instead of contiguous KV cache, we gather K/V from scattered pages.\n",
    "    \"\"\"\n",
    "    batch, _, num_heads, head_dim = q.shape\n",
    "    max_pages = page_table.shape[1]\n",
    "    scale = 1.0 / jnp.sqrt(head_dim)\n",
    "\n",
    "    def single_sequence_attention(q_single, page_indices, seq_len):\n",
    "        \"\"\"Attention for one sequence.\"\"\"\n",
    "        # q_single: (1, num_heads, head_dim)\n",
    "        # page_indices: (max_pages_per_seq,)\n",
    "\n",
    "        # Gather K/V pages for this sequence\n",
    "        # k_seq: (max_pages, page_size, num_heads, head_dim)\n",
    "        k_seq = k_pages[page_indices]\n",
    "        v_seq = v_pages[page_indices]\n",
    "\n",
    "        # Reshape to (max_pages * page_size, num_heads, head_dim)\n",
    "        max_len = max_pages * page_size\n",
    "        k_flat = k_seq.reshape(max_len, num_heads, head_dim)\n",
    "        v_flat = v_seq.reshape(max_len, num_heads, head_dim)\n",
    "\n",
    "        # Compute attention scores: (num_heads, 1, max_len)\n",
    "        scores = jnp.einsum('qhd,khd->hqk', q_single[0], k_flat) * scale\n",
    "\n",
    "        # Mask positions beyond actual sequence length\n",
    "        positions = jnp.arange(max_len)[None, None, :]\n",
    "        mask = positions < seq_len\n",
    "        scores = jnp.where(mask, scores, -1e9)\n",
    "\n",
    "        weights = jax.nn.softmax(scores, axis=-1)\n",
    "        output = jnp.einsum('hqk,khd->qhd', weights, v_flat)\n",
    "        return output\n",
    "\n",
    "    # vmap over batch\n",
    "    outputs = jax.vmap(single_sequence_attention)(q, page_table, seq_lens)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd59ca69",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39efef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Example: Simulate paged KV cache serving ----\n",
    "\n",
    "print(\"=== Paged Attention Demo ===\\n\")\n",
    "\n",
    "# Configuration\n",
    "num_pages = 64\n",
    "page_size = 16\n",
    "num_heads = 8\n",
    "head_dim = 64\n",
    "batch_size = 4\n",
    "max_pages_per_seq = 8  # supports up to 128 tokens per sequence\n",
    "\n",
    "# Initialize paged cache\n",
    "cache = PagedKVCache(num_pages, page_size, num_heads, head_dim)\n",
    "\n",
    "key = jax.random.PRNGKey(42)\n",
    "\n",
    "# Simulate 4 sequences with different lengths\n",
    "actual_seq_lens = jnp.array([45, 80, 30, 100])\n",
    "\n",
    "# Allocate pages and fill with dummy KV data\n",
    "page_table = jnp.zeros((batch_size, max_pages_per_seq), dtype=jnp.int32)\n",
    "\n",
    "for b in range(batch_size):\n",
    "    num_needed = int(jnp.ceil(actual_seq_lens[b] / page_size))\n",
    "    for p in range(num_needed):\n",
    "        phys_page = cache.allocate_page()\n",
    "        page_table = page_table.at[b, p].set(phys_page)\n",
    "\n",
    "        # Fill page with random KV data\n",
    "        key, k1, k2 = jax.random.split(key, 3)\n",
    "        cache.k_pages = cache.k_pages.at[phys_page].set(\n",
    "            jax.random.normal(k1, (page_size, num_heads, head_dim))\n",
    "        )\n",
    "        cache.v_pages = cache.v_pages.at[phys_page].set(\n",
    "            jax.random.normal(k2, (page_size, num_heads, head_dim))\n",
    "        )\n",
    "\n",
    "print(f\"Page table:\\n{page_table}\")\n",
    "print(f\"Sequence lengths: {actual_seq_lens}\")\n",
    "print(f\"Pages allocated: {num_pages - len(cache.free_pages)}/{num_pages}\")\n",
    "print(f\"Pages free: {len(cache.free_pages)}/{num_pages}\")\n",
    "\n",
    "# Run paged attention for current query tokens\n",
    "key, qkey = jax.random.split(key)\n",
    "q = jax.random.normal(qkey, (batch_size, 1, num_heads, head_dim))\n",
    "\n",
    "output = paged_attention(\n",
    "    q, cache.k_pages, cache.v_pages,\n",
    "    page_table, actual_seq_lens,\n",
    "    page_size=page_size\n",
    ")\n",
    "print(f\"\\nPaged attention output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377f1209",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22294153",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Memory Comparison ----\n",
    "\n",
    "max_seq_len = max_pages_per_seq * page_size  # 128\n",
    "standard_memory = batch_size * max_seq_len * num_heads * head_dim * 2 * 4  # K+V, float32\n",
    "paged_memory = (num_pages - len(cache.free_pages)) * page_size * num_heads * head_dim * 2 * 4\n",
    "\n",
    "print(f\"\\n=== Memory Comparison ===\")\n",
    "print(f\"Standard KV cache: {standard_memory / 1024:.1f} KB (pre-allocates max_seq_len)\")\n",
    "print(f\"Paged KV cache:    {paged_memory / 1024:.1f} KB (allocates only what's needed)\")\n",
    "print(f\"Memory saved:      {(1 - paged_memory / standard_memory) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79bcb3d",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf70ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Copy-on-Write for Beam Search ----\n",
    "# Multiple beams can share prefix pages — only divergent pages are copied\n",
    "\n",
    "def fork_sequence(page_table, src_seq_idx, dst_seq_idx):\n",
    "    \"\"\"\n",
    "    Fork a sequence for beam search: dst shares src's pages (CoW).\n",
    "    In a real system, pages are only physically copied when modified.\n",
    "    \"\"\"\n",
    "    return page_table.at[dst_seq_idx].set(page_table[src_seq_idx])\n",
    "\n",
    "# Beam search example: fork sequence 0 into sequence 3\n",
    "print(f\"\\n=== Copy-on-Write Fork ===\")\n",
    "print(f\"Before fork - seq 3 pages: {page_table[3]}\")\n",
    "page_table_forked = fork_sequence(page_table, 0, 3)\n",
    "print(f\"After fork  - seq 3 pages: {page_table_forked[3]} (shares seq 0's pages)\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
