{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acc6feba",
   "metadata": {},
   "source": [
    "pmap: Parallel Map across Devices\n",
    "==================================\n",
    "\n",
    "Key concepts:\n",
    "- pmap replicates a function across multiple devices (GPUs/TPUs).\n",
    "- Each device gets a slice of the data (SPMD: Single Program Multiple Data).\n",
    "- Use jax.lax.psum, pmean, pmax for cross-device communication (collectives).\n",
    "- pmap is the classic approach; for newer code, consider jax.sharding (see 05).\n",
    "\n",
    "Note: These examples will run on CPU (simulating 1 device) if no GPU/TPU\n",
    "is available. Set XLA_FLAGS to simulate multiple devices:\n",
    "  XLA_FLAGS='--xla_force_host_platform_device_count=4' python 04_pmap.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992ac93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import os\n",
    "\n",
    "# Simulate 4 devices on CPU (remove this if you have real GPUs/TPUs)\n",
    "os.environ['XLA_FLAGS'] = '--xla_force_host_platform_device_count=4'\n",
    "\n",
    "print(f\"Number of devices: {jax.device_count()}\")\n",
    "print(f\"Devices: {jax.devices()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3ce138",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a65f241",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Basic pmap ----\n",
    "# Replicate a function across devices, each getting a data shard\n",
    "\n",
    "def square(x):\n",
    "    return x ** 2\n",
    "\n",
    "# Input shape: (num_devices, ...) — first axis is the device axis\n",
    "x = jnp.arange(4.0)  # [0, 1, 2, 3] — one value per device\n",
    "result = jax.pmap(square)(x)\n",
    "print(\"pmap square:\", result)  # [0, 1, 4, 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbd14cd",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ae7d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Data-parallel training with pmap ----\n",
    "\n",
    "def loss_fn(params, x_batch, y_batch):\n",
    "    \"\"\"Per-device loss: each device processes its local batch.\"\"\"\n",
    "    pred = jnp.dot(x_batch, params['w']) + params['b']\n",
    "    return jnp.mean((pred - y_batch) ** 2)\n",
    "\n",
    "@jax.pmap\n",
    "def train_step(params, x_batch, y_batch):\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(params, x_batch, y_batch)\n",
    "\n",
    "    # Average gradients across all devices\n",
    "    grads = jax.lax.pmean(grads, axis_name='devices')\n",
    "    loss = jax.lax.pmean(loss, axis_name='devices')\n",
    "\n",
    "    # SGD update (same on every device since grads are synchronized)\n",
    "    new_params = jax.tree.map(lambda p, g: p - 0.01 * g, params, grads)\n",
    "    return new_params, loss\n",
    "\n",
    "# Need to specify axis_name for collectives\n",
    "# train_step = jax.pmap(...)  # This was redundant and causing issues in the script\n",
    "\n",
    "# Setup: replicate params across devices, shard data\n",
    "n_devices = jax.device_count()\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "params = {\n",
    "    'w': jnp.zeros(3),\n",
    "    'b': jnp.float32(0.0),\n",
    "}\n",
    "# Replicate params to all devices\n",
    "replicated_params = jax.tree.map(lambda x: jnp.stack([x] * n_devices), params)\n",
    "print(\"Replicated w shape:\", replicated_params['w'].shape)  # (4, 3)\n",
    "\n",
    "# Shard data across devices: shape (n_devices, per_device_batch, features)\n",
    "total_batch = 32\n",
    "per_device_batch = total_batch // n_devices\n",
    "X = jax.random.normal(key, (n_devices, per_device_batch, 3))\n",
    "Y = jax.random.normal(key, (n_devices, per_device_batch))\n",
    "\n",
    "(loss, grads) = train_step(replicated_params, X, Y)\n",
    "print(\"Per-device losses:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d675614",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea145af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Collectives: psum, pmean, pmax, pmin ----\n",
    "\n",
    "@jax.pmap(axis_name='i')\n",
    "def demonstrate_collectives(x):\n",
    "    total = jax.lax.psum(x, axis_name='i')     # sum across devices\n",
    "    mean = jax.lax.pmean(x, axis_name='i')      # mean across devices\n",
    "    maximum = jax.lax.pmax(x, axis_name='i')    # max across devices\n",
    "    return {'sum': total, 'mean': mean, 'max': maximum}\n",
    "\n",
    "x = jnp.array([1.0, 2.0, 3.0, 4.0])  # one value per device\n",
    "results = demonstrate_collectives(x)\n",
    "print(f\"\\nCollectives on [1,2,3,4]:\")\n",
    "print(f\"  psum: {results['sum']}\")    # [10, 10, 10, 10]\n",
    "print(f\"  pmean: {results['mean']}\")  # [2.5, 2.5, 2.5, 2.5]\n",
    "print(f\"  pmax: {results['max']}\")    # [4, 4, 4, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b17380",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553feed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. pmap + vmap composition ----\n",
    "# pmap across devices, vmap across batch within each device\n",
    "\n",
    "def single_predict(w, x):\n",
    "    return jnp.dot(w, x)\n",
    "\n",
    "# Inner vmap: batch within device; Outer pmap: across devices\n",
    "distributed_predict = jax.pmap(\n",
    "    jax.vmap(single_predict, in_axes=(None, 0)),\n",
    "    in_axes=(0, 0)\n",
    ")\n",
    "\n",
    "W = jax.random.normal(key, (n_devices, 3))           # one weight per device\n",
    "X = jax.random.normal(key, (n_devices, 8, 3))        # 8 samples per device\n",
    "\n",
    "preds = distributed_predict(W, X)\n",
    "print(f\"\\npmap+vmap predictions shape: {preds.shape}\")  # (4, 8)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
