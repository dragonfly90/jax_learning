{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82b416f1",
   "metadata": {},
   "source": [
    "Neural Networks with Flax (Linen API)\n",
    "======================================\n",
    "\n",
    "Key concepts:\n",
    "- Flax is the most popular neural network library for JAX.\n",
    "- nn.Module: Define layers and models declaratively.\n",
    "- params = model.init(key, x): Initialize parameters (returns a pytree).\n",
    "- model.apply(params, x): Forward pass (pure function, no hidden state).\n",
    "- Optax: Gradient-based optimizers for JAX.\n",
    "\n",
    "Install: pip install flax optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ce4e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import optax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494511b6",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04945ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Define a simple MLP ----\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Multi-layer perceptron.\"\"\"\n",
    "    hidden_dim: int = 128\n",
    "    output_dim: int = 10\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, training: bool = False):\n",
    "        x = nn.Dense(self.hidden_dim)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dropout(rate=0.1)(x, deterministic=not training)\n",
    "        x = nn.Dense(self.hidden_dim)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(self.output_dim)(x)\n",
    "        return x\n",
    "\n",
    "# Initialize\n",
    "model = MLP(hidden_dim=64, output_dim=10)\n",
    "key = jax.random.PRNGKey(0)\n",
    "dummy_input = jnp.ones((1, 784))  # e.g., flattened MNIST\n",
    "\n",
    "# init returns a dict of parameters\n",
    "variables = model.init(key, dummy_input)\n",
    "params = variables['params']\n",
    "\n",
    "print(\"Parameter tree:\")\n",
    "print(jax.tree.map(lambda x: x.shape, params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979a978f",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0a7c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Forward pass ----\n",
    "\n",
    "# Inference (no dropout)\n",
    "logits = model.apply({'params': params}, dummy_input)\n",
    "print(f\"\\nOutput shape: {logits.shape}\")\n",
    "\n",
    "# Training (with dropout â€” need to pass an RNG key)\n",
    "logits_train = model.apply(\n",
    "    {'params': params},\n",
    "    dummy_input,\n",
    "    training=True,\n",
    "    rngs={'dropout': jax.random.PRNGKey(1)}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b414b5b0",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a57c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Training loop with Optax ----\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = optax.adam(learning_rate=1e-3)\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "def cross_entropy_loss(params, x, y, key):\n",
    "    logits = model.apply(\n",
    "        {'params': params}, x,\n",
    "        training=True,\n",
    "        rngs={'dropout': key}\n",
    "    )\n",
    "    one_hot = jax.nn.one_hot(y, 10)\n",
    "    return -jnp.mean(jnp.sum(one_hot * jax.nn.log_softmax(logits), axis=-1))\n",
    "\n",
    "@jax.jit\n",
    "def train_step(params, opt_state, x, y, key):\n",
    "    loss, grads = jax.value_and_grad(cross_entropy_loss)(params, x, y, key)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss\n",
    "\n",
    "# Simulate training\n",
    "key = jax.random.PRNGKey(42)\n",
    "for step in range(100):\n",
    "    key, subkey, data_key = jax.random.split(key, 3)\n",
    "    # Fake data\n",
    "    x = jax.random.normal(data_key, (32, 784))\n",
    "    y = jax.random.randint(subkey, (32,), 0, 10)\n",
    "\n",
    "    key, dropout_key = jax.random.split(key)\n",
    "    params, opt_state, loss = train_step(params, opt_state, x, y, dropout_key)\n",
    "\n",
    "    if step % 20 == 0:\n",
    "        print(f\"Step {step}: loss={loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe425d9",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a613c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. CNN Example ----\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \"\"\"Simple CNN for image classification.\"\"\"\n",
    "    num_classes: int = 10\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, training: bool = False):\n",
    "        # x shape: (batch, height, width, channels)\n",
    "        x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "\n",
    "        x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "\n",
    "        x = x.reshape(x.shape[0], -1)  # flatten\n",
    "        x = nn.Dense(256)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(self.num_classes)(x)\n",
    "        return x\n",
    "\n",
    "cnn = CNN()\n",
    "dummy_image = jnp.ones((1, 28, 28, 1))\n",
    "cnn_params = cnn.init(key, dummy_image)['params']\n",
    "print(\"\\nCNN parameter shapes:\")\n",
    "print(jax.tree.map(lambda x: x.shape, cnn_params))\n",
    "\n",
    "logits = cnn.apply({'params': cnn_params}, dummy_image)\n",
    "print(f\"CNN output shape: {logits.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed7736f",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3cf121",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Batch Normalization (with state) ----\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    features: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, training: bool = False):\n",
    "        residual = x\n",
    "        x = nn.BatchNorm(use_running_average=not training)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(self.features)(x)\n",
    "        x = nn.BatchNorm(use_running_average=not training)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(self.features)(x)\n",
    "        if residual.shape != x.shape:\n",
    "            residual = nn.Dense(self.features)(residual)\n",
    "        return x + residual\n",
    "\n",
    "block = ResBlock(features=64)\n",
    "variables = block.init(key, jnp.ones((4, 64)), training=True)\n",
    "print(\"\\nResBlock variable collections:\", list(variables.keys()))\n",
    "# 'params' for weights, 'batch_stats' for running mean/var\n",
    "\n",
    "# Forward with mutable batch stats\n",
    "output, updates = block.apply(\n",
    "    variables,\n",
    "    jnp.ones((4, 64)),\n",
    "    training=True,\n",
    "    mutable=['batch_stats']\n",
    ")\n",
    "print(f\"ResBlock output shape: {output.shape}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
