{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cde16d0d",
   "metadata": {},
   "source": [
    "jax.value_and_grad: Computing Values and Gradients Together\n",
    "============================================================\n",
    "\n",
    "Key concepts:\n",
    "- jax.value_and_grad returns BOTH the function output and its gradient.\n",
    "- More efficient than calling jax.grad separately (one forward+backward pass).\n",
    "- argnums controls which arguments to differentiate w.r.t.\n",
    "- has_aux=True lets you return extra outputs (metrics, intermediates).\n",
    "- This is the workhorse of every JAX training loop.\n",
    "\n",
    "Comparison:\n",
    "  jax.grad(f)(x)            -> gradient only (discards function value)\n",
    "  jax.value_and_grad(f)(x)  -> (value, gradient) in one pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51ab35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17584614",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af42956",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Basic Usage ----\n",
    "# value_and_grad wraps a function to return (output, gradient)\n",
    "\n",
    "def f(x):\n",
    "    return jnp.sum(x ** 2)\n",
    "\n",
    "# jax.grad — returns only the gradient\n",
    "grad_fn = jax.grad(f)\n",
    "print(\"=== 1. Basic Usage ===\")\n",
    "print(f\"grad only:      {grad_fn(jnp.array([1.0, 2.0, 3.0]))}\")\n",
    "\n",
    "# jax.value_and_grad — returns (value, gradient) together\n",
    "val_grad_fn = jax.value_and_grad(f)\n",
    "value, grad = val_grad_fn(jnp.array([1.0, 2.0, 3.0]))\n",
    "print(f\"value:          {value}\")        # 1 + 4 + 9 = 14.0\n",
    "print(f\"gradient:       {grad}\")         # [2, 4, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f49b248",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48d5c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. argnums — Choosing Which Arguments to Differentiate ----\n",
    "# By default, differentiates w.r.t. argument 0.\n",
    "\n",
    "def loss_fn(w, b, x, y):\n",
    "    pred = x @ w + b\n",
    "    return jnp.mean((pred - y) ** 2)\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "w = jax.random.normal(key, (4, 2))\n",
    "b = jnp.zeros(2)\n",
    "x = jax.random.normal(key, (8, 4))\n",
    "y = jax.random.normal(key, (8, 2))\n",
    "\n",
    "print(\"\\n=== 2. argnums ===\")\n",
    "\n",
    "# Differentiate w.r.t. arg 0 (w) only — default\n",
    "loss, dw = jax.value_and_grad(loss_fn)(w, b, x, y)\n",
    "print(f\"argnums=0 (default): loss={loss:.4f}, dw shape={dw.shape}\")\n",
    "\n",
    "# Differentiate w.r.t. arg 1 (b) only\n",
    "loss, db = jax.value_and_grad(loss_fn, argnums=1)(w, b, x, y)\n",
    "print(f\"argnums=1:           loss={loss:.4f}, db shape={db.shape}\")\n",
    "\n",
    "# Differentiate w.r.t. both w and b — returns tuple of gradients\n",
    "loss, (dw, db) = jax.value_and_grad(loss_fn, argnums=(0, 1))(w, b, x, y)\n",
    "print(f\"argnums=(0,1):       loss={loss:.4f}, dw shape={dw.shape}, db shape={db.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a012653",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591cbb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. has_aux — Returning Extra Outputs ----\n",
    "# Sometimes you want to return metrics, logits, or intermediates alongside the loss.\n",
    "# has_aux=True tells JAX: the function returns (loss, aux), differentiate only the loss.\n",
    "\n",
    "def loss_with_metrics(params, x, y):\n",
    "    pred = x @ params['w'] + params['b']\n",
    "    loss = jnp.mean((pred - y) ** 2)\n",
    "    # Return extra info that we don't differentiate\n",
    "    metrics = {\n",
    "        'mse': loss,\n",
    "        'pred_mean': jnp.mean(pred),\n",
    "        'pred_std': jnp.std(pred),\n",
    "    }\n",
    "    return loss, metrics  # (differentiable, auxiliary)\n",
    "\n",
    "params = {'w': w, 'b': b}\n",
    "\n",
    "print(\"\\n=== 3. has_aux=True ===\")\n",
    "\n",
    "# Without has_aux: JAX would try to differentiate the metrics dict (error!)\n",
    "# With has_aux: JAX differentiates only the first output\n",
    "(loss, metrics), grads = jax.value_and_grad(loss_with_metrics, has_aux=True)(params, x, y)\n",
    "print(f\"loss:      {loss:.4f}\")\n",
    "print(f\"pred_mean: {metrics['pred_mean']:.4f}\")\n",
    "print(f\"pred_std:  {metrics['pred_std']:.4f}\")\n",
    "print(f\"grad keys: {list(grads.keys())}\")  # same structure as params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d6e980",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b08dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. With Pytrees (Nested Params) ----\n",
    "# value_and_grad works naturally with pytrees — gradients have the same structure.\n",
    "\n",
    "def mlp_loss(params, x, y):\n",
    "    # Two-layer MLP\n",
    "    h = jnp.tanh(x @ params['layer1']['w'] + params['layer1']['b'])\n",
    "    pred = h @ params['layer2']['w'] + params['layer2']['b']\n",
    "    return jnp.mean((pred - y) ** 2)\n",
    "\n",
    "params = {\n",
    "    'layer1': {'w': jax.random.normal(key, (4, 8)), 'b': jnp.zeros(8)},\n",
    "    'layer2': {'w': jax.random.normal(key, (8, 2)), 'b': jnp.zeros(2)},\n",
    "}\n",
    "\n",
    "print(\"\\n=== 4. Pytree Gradients ===\")\n",
    "loss, grads = jax.value_and_grad(mlp_loss)(params, x, y)\n",
    "print(f\"loss: {loss:.4f}\")\n",
    "print(f\"grad structure:\")\n",
    "for layer_name, layer_grads in grads.items():\n",
    "    for param_name, g in layer_grads.items():\n",
    "        print(f\"  {layer_name}/{param_name}: shape={g.shape}, norm={jnp.linalg.norm(g):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65234f4",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1303b02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Combining with jax.jit ----\n",
    "# value_and_grad composes with jit for fast compiled training steps.\n",
    "\n",
    "import optax\n",
    "\n",
    "print(\"\\n=== 5. JIT + value_and_grad Training Loop ===\")\n",
    "\n",
    "optimizer = optax.adam(1e-2)\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "@jax.jit\n",
    "def train_step(params, opt_state, x, y):\n",
    "    loss, grads = jax.value_and_grad(mlp_loss)(params, x, y)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss\n",
    "\n",
    "for step in range(200):\n",
    "    params, opt_state, loss = train_step(params, opt_state, x, y)\n",
    "    if step % 40 == 0:\n",
    "        print(f\"  Step {step:3d}: loss={loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5143729b",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6051c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. JIT + has_aux — The Complete Training Pattern ----\n",
    "# The most common pattern in real JAX training code.\n",
    "\n",
    "print(\"\\n=== 6. Complete Training Pattern ===\")\n",
    "\n",
    "def full_loss_fn(params, x, y):\n",
    "    h = jnp.tanh(x @ params['layer1']['w'] + params['layer1']['b'])\n",
    "    logits = h @ params['layer2']['w'] + params['layer2']['b']\n",
    "    loss = jnp.mean((logits - y) ** 2)\n",
    "    return loss, {'logits': logits, 'loss': loss}\n",
    "\n",
    "# Re-init\n",
    "params = {\n",
    "    'layer1': {'w': jax.random.normal(key, (4, 8)), 'b': jnp.zeros(8)},\n",
    "    'layer2': {'w': jax.random.normal(key, (8, 2)), 'b': jnp.zeros(2)},\n",
    "}\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "@jax.jit\n",
    "def train_step_full(params, opt_state, x, y):\n",
    "    (loss, aux), grads = jax.value_and_grad(full_loss_fn, has_aux=True)(params, x, y)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss, aux\n",
    "\n",
    "for step in range(200):\n",
    "    params, opt_state, loss, aux = train_step_full(params, opt_state, x, y)\n",
    "    if step % 40 == 0:\n",
    "        print(f\"  Step {step:3d}: loss={loss:.4f}, pred_mean={jnp.mean(aux['logits']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9982ca91",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba09fdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. Higher-Order: Hessian-Vector Products ----\n",
    "# value_and_grad composes with other transforms like grad (for second derivatives).\n",
    "\n",
    "print(\"\\n=== 7. Higher-Order Derivatives ===\")\n",
    "\n",
    "def scalar_fn(x):\n",
    "    return jnp.sum(jnp.sin(x) * x ** 2)\n",
    "\n",
    "x = jnp.array([1.0, 2.0, 3.0])\n",
    "\n",
    "# First derivative\n",
    "val, grad1 = jax.value_and_grad(scalar_fn)(x)\n",
    "print(f\"f(x)  = {val:.4f}\")\n",
    "print(f\"f'(x) = {grad1}\")\n",
    "\n",
    "# Second derivative (Hessian diagonal) via composing grad\n",
    "hessian_diag_fn = jax.value_and_grad(lambda x: jnp.sum(jax.grad(scalar_fn)(x) ** 2))\n",
    "val2, grad2 = hessian_diag_fn(x)\n",
    "print(f\"||f'(x)||^2 = {val2:.4f}\")\n",
    "print(f\"gradient of ||f'(x)||^2 = {grad2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfbc59e",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d371b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. Summary ----\n",
    "\n",
    "print(\"\"\"\n",
    "=== Summary ===\n",
    "\n",
    "jax.value_and_grad(fn)                    -> (value, grad_arg0)\n",
    "jax.value_and_grad(fn, argnums=1)         -> (value, grad_arg1)\n",
    "jax.value_and_grad(fn, argnums=(0,1))     -> (value, (grad_arg0, grad_arg1))\n",
    "jax.value_and_grad(fn, has_aux=True)      -> ((value, aux), grad)\n",
    "\n",
    "Common training pattern:\n",
    "  @jax.jit\n",
    "  def train_step(params, opt_state, x, y):\n",
    "      (loss, aux), grads = jax.value_and_grad(loss_fn, has_aux=True)(params, x, y)\n",
    "      updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "      params = optax.apply_updates(params, updates)\n",
    "      return params, opt_state, loss, aux\n",
    "\n",
    "Key points:\n",
    "- Always prefer value_and_grad over separate grad + function call\n",
    "- has_aux=True for returning metrics/intermediates alongside loss\n",
    "- Gradients match the pytree structure of the differentiated argument\n",
    "- Composes naturally with jit, vmap, and other JAX transforms\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
