{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5861e585",
   "metadata": {},
   "source": [
    "JAX Basics: Arrays, JIT compilation, and Automatic Differentiation\n",
    "==================================================================\n",
    "\n",
    "Key concepts:\n",
    "- jax.numpy: Drop-in replacement for NumPy that runs on GPU/TPU\n",
    "- jit: Just-in-time compilation via XLA for speed\n",
    "- grad: Automatic differentiation for computing gradients\n",
    "- JAX arrays are immutable (no in-place mutation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e60593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a32e3b",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935d14eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. JAX Arrays (DeviceArray) ----\n",
    "# JAX arrays work like NumPy but are immutable and can live on GPU/TPU\n",
    "\n",
    "x = jnp.array([1.0, 2.0, 3.0])\n",
    "y = jnp.ones((3, 3))\n",
    "z = jnp.dot(y, x)\n",
    "print(\"Dot product:\", z)\n",
    "\n",
    "# Immutability: you cannot do x[0] = 10.0\n",
    "# Instead, use functional updates:\n",
    "x_updated = x.at[0].set(10.0)\n",
    "print(\"Original x:\", x)\n",
    "print(\"Updated x:\", x_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9c676c",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576a177a",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Random Numbers ----\n",
    "# JAX uses explicit PRNG keys (no global state like NumPy)\n",
    "\n",
    "key = jax.random.PRNGKey(42)\n",
    "key1, key2 = jax.random.split(key)\n",
    "random_vector = jax.random.normal(key1, shape=(5,))\n",
    "print(\"Random vector:\", random_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62a7a36",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84a4723",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. JIT Compilation ----\n",
    "# jax.jit compiles a function with XLA for faster execution\n",
    "\n",
    "def slow_fn(x):\n",
    "    for _ in range(5):\n",
    "        x = x @ x\n",
    "    return x\n",
    "\n",
    "fast_fn = jax.jit(slow_fn)\n",
    "\n",
    "mat = jax.random.normal(key2, (100, 100))\n",
    "# First call compiles (slower), subsequent calls are fast\n",
    "result = fast_fn(mat)\n",
    "print(\"JIT result shape:\", result.shape)\n",
    "\n",
    "# Can also use as a decorator:\n",
    "@jax.jit\n",
    "def another_fast_fn(x, y):\n",
    "    return jnp.sin(x) + jnp.cos(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b8c043",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed60e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Automatic Differentiation with grad ----\n",
    "# jax.grad computes gradients of scalar-valued functions\n",
    "\n",
    "def loss_fn(w, x, y):\n",
    "    \"\"\"Simple MSE loss.\"\"\"\n",
    "    pred = jnp.dot(x, w)\n",
    "    return jnp.mean((pred - y) ** 2)\n",
    "\n",
    "key3, key4 = jax.random.split(key1)\n",
    "w = jax.random.normal(key3, (3,))\n",
    "x = jax.random.normal(key4, (10, 3))\n",
    "y = jnp.ones(10)\n",
    "\n",
    "# Gradient with respect to the first argument (w)\n",
    "grad_fn = jax.grad(loss_fn)\n",
    "grads = grad_fn(w, x, y)\n",
    "print(\"Gradients shape:\", grads.shape)\n",
    "\n",
    "# value_and_grad returns both the loss value and the gradient\n",
    "val_grad_fn = jax.value_and_grad(loss_fn)\n",
    "loss_val, grads = val_grad_fn(w, x, y)\n",
    "print(f\"Loss: {loss_val:.4f}, Grad norm: {jnp.linalg.norm(grads):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcec82a",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69d8208",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Higher-order Differentiation ----\n",
    "# You can compose grad to get higher-order derivatives\n",
    "\n",
    "def f(x):\n",
    "    return jnp.sin(x)\n",
    "\n",
    "df = jax.grad(f)       # first derivative: cos(x)\n",
    "ddf = jax.grad(df)     # second derivative: -sin(x)\n",
    "\n",
    "x0 = 1.0\n",
    "print(f\"f(x)={f(x0):.4f}, f'(x)={df(x0):.4f}, f''(x)={ddf(x0):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b83d07",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f973e274",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Jacobians and Hessians ----\n",
    "\n",
    "def vector_fn(x):\n",
    "    return jnp.array([x[0] ** 2 + x[1], x[0] * x[1] ** 3])\n",
    "\n",
    "# Full Jacobian matrix\n",
    "jacobian = jax.jacobian(vector_fn)(jnp.array([1.0, 2.0]))\n",
    "print(\"Jacobian:\\n\", jacobian)\n",
    "\n",
    "# Hessian of a scalar function\n",
    "def scalar_fn(x):\n",
    "    return jnp.sum(x ** 3)\n",
    "\n",
    "hessian = jax.hessian(scalar_fn)(jnp.array([1.0, 2.0, 3.0]))\n",
    "print(\"Hessian:\\n\", hessian)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
