{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5868bf9f",
   "metadata": {},
   "source": [
    "vmap: Vectorized Map (Auto-Batching)\n",
    "====================================\n",
    "\n",
    "Key concepts:\n",
    "- vmap transforms a function that operates on single examples into one\n",
    "  that operates on batches, without manually writing batch dimensions.\n",
    "- It's a compiler transformation, not a Python loop â€” so it's fast.\n",
    "- in_axes / out_axes control which dimensions to map over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc737b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca583c5",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a33cb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Basic vmap ----\n",
    "# Suppose we have a function that works on a single vector:\n",
    "\n",
    "def predict(weights, x):\n",
    "    \"\"\"Single prediction: dot product + bias.\"\"\"\n",
    "    return jnp.dot(weights[:-1], x) + weights[-1]\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "weights = jax.random.normal(key, (4,))  # 3 weights + 1 bias\n",
    "single_x = jnp.array([1.0, 2.0, 3.0])\n",
    "\n",
    "print(\"Single prediction:\", predict(weights, single_x))\n",
    "\n",
    "# To predict on a BATCH of inputs, use vmap:\n",
    "batch_x = jax.random.normal(key, (8, 3))  # batch of 8\n",
    "\n",
    "# Map over axis 0 of x, but broadcast weights (None = don't map)\n",
    "batch_predict = jax.vmap(predict, in_axes=(None, 0))\n",
    "predictions = batch_predict(weights, batch_x)\n",
    "print(\"Batch predictions shape:\", predictions.shape)  # (8,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e9da11",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2612030",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. in_axes and out_axes ----\n",
    "# in_axes specifies which axis of each argument to vectorize over\n",
    "# None means \"broadcast this argument\" (don't map over it)\n",
    "\n",
    "def pairwise_distance(a, b):\n",
    "    \"\"\"Euclidean distance between two vectors.\"\"\"\n",
    "    return jnp.sqrt(jnp.sum((a - b) ** 2))\n",
    "\n",
    "points_a = jax.random.normal(key, (5, 3))\n",
    "points_b = jax.random.normal(key, (5, 3))\n",
    "\n",
    "# Map over axis 0 of both a and b\n",
    "batched_dist = jax.vmap(pairwise_distance, in_axes=(0, 0))\n",
    "distances = batched_dist(points_a, points_b)\n",
    "print(\"Pairwise distances:\", distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89cd973",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b058ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Nested vmap for pairwise computations ----\n",
    "# Compute ALL pairwise distances (5x5 distance matrix)\n",
    "\n",
    "# Outer vmap: iterate over rows of points_a\n",
    "# Inner vmap: for each row of a, iterate over all rows of b\n",
    "all_pairs_dist = jax.vmap(\n",
    "    jax.vmap(pairwise_distance, in_axes=(None, 0)),\n",
    "    in_axes=(0, None)\n",
    ")\n",
    "dist_matrix = all_pairs_dist(points_a, points_b)\n",
    "print(\"Distance matrix shape:\", dist_matrix.shape)  # (5, 5)\n",
    "print(\"Distance matrix:\\n\", dist_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7064406b",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a172d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. vmap + grad: Per-sample gradients ----\n",
    "# Extremely useful for ML: compute gradients for each sample individually\n",
    "\n",
    "def single_loss(w, x, y):\n",
    "    pred = jnp.dot(w, x)\n",
    "    return (pred - y) ** 2\n",
    "\n",
    "# grad with respect to w for a single sample\n",
    "single_grad = jax.grad(single_loss)\n",
    "\n",
    "# vmap over samples (axis 0 of x and y), broadcast w\n",
    "per_sample_grads = jax.vmap(single_grad, in_axes=(None, 0, 0))\n",
    "\n",
    "w = jax.random.normal(key, (3,))\n",
    "X = jax.random.normal(key, (16, 3))\n",
    "Y = jax.random.normal(key, (16,))\n",
    "\n",
    "grads = per_sample_grads(w, X, Y)\n",
    "print(\"Per-sample gradients shape:\", grads.shape)  # (16, 3)\n",
    "print(\"Mean gradient:\", jnp.mean(grads, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9354b514",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886bbadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. vmap with jit for performance ----\n",
    "# Always jit the outer function for best performance\n",
    "\n",
    "@jax.jit\n",
    "def fast_batch_predict(weights, batch_x):\n",
    "    return jax.vmap(predict, in_axes=(None, 0))(weights, batch_x)\n",
    "\n",
    "result = fast_batch_predict(weights, batch_x)\n",
    "print(\"JIT + vmap result shape:\", result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0351793f",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defccaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Practical example: batched matrix-vector multiply ----\n",
    "# Given a batch of matrices and a batch of vectors, multiply each pair\n",
    "\n",
    "def matvec(A, x):\n",
    "    return A @ x\n",
    "\n",
    "batch_A = jax.random.normal(key, (32, 4, 4))\n",
    "batch_x = jax.random.normal(key, (32, 4))\n",
    "\n",
    "batched_matvec = jax.vmap(matvec)\n",
    "results = batched_matvec(batch_A, batch_x)\n",
    "print(\"Batched matvec result shape:\", results.shape)  # (32, 4)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
