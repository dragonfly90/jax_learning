{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "763b7eb0",
   "metadata": {},
   "source": [
    "TPU Training with JAX\n",
    "======================\n",
    "\n",
    "Key concepts:\n",
    "- JAX is designed for TPUs — most code works on TPU without changes.\n",
    "- TPU pods use Mesh + NamedSharding for multi-host parallelism.\n",
    "- Common strategies: data parallelism, FSDP, tensor parallelism, pipeline.\n",
    "- bfloat16 is native on TPU and preferred over float16.\n",
    "- TPU-specific optimizations: donate_buffers, prefetch, XLA flags.\n",
    "\n",
    "Note: This file demonstrates patterns for TPU training. On CPU/GPU it\n",
    "will run but with simulated devices. For actual TPU usage, run on\n",
    "Google Cloud TPU VMs or Colab TPU runtimes.\n",
    "\n",
    "Setup on TPU VM:\n",
    "  pip install jax[tpu] -f https://storage.googleapis.com/jax-releases/libtpu_releases.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32845b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.sharding import Mesh, PartitionSpec as P, NamedSharding\n",
    "import optax\n",
    "import flax.linen as nn\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Simulate 8 devices for demonstration\n",
    "if 'TPU_CHIPS_PER_HOST_BOUNDS' not in os.environ:\n",
    "    os.environ['XLA_FLAGS'] = '--xla_force_host_platform_device_count=8'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b5c4e1",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe16be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. TPU Device Discovery ----\n",
    "\n",
    "print(\"=== Device Info ===\")\n",
    "print(f\"Backend: {jax.default_backend()}\")\n",
    "print(f\"Num devices: {jax.device_count()}\")\n",
    "print(f\"Local devices: {jax.local_device_count()}\")\n",
    "print(f\"Process index: {jax.process_index()}\")\n",
    "print(f\"Process count: {jax.process_count()}\")\n",
    "for d in jax.devices()[:4]:\n",
    "    print(f\"  {d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9116e9e8",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f065489",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Creating Meshes for TPU Topologies ----\n",
    "# TPU v4 pods have 2D/3D physical topologies; meshes map to these\n",
    "\n",
    "devices = np.array(jax.devices())\n",
    "\n",
    "# 1D mesh: pure data parallelism\n",
    "mesh_dp = Mesh(devices, axis_names=('dp',))\n",
    "\n",
    "# 2D mesh: data + model parallelism (e.g., 4x2 for 8 devices)\n",
    "mesh_2d = Mesh(devices.reshape(4, 2), axis_names=('dp', 'mp'))\n",
    "\n",
    "# For large TPU pods (e.g., v4-128 = 64 chips):\n",
    "# mesh = Mesh(devices.reshape(16, 4), axis_names=('dp', 'mp'))\n",
    "# or 3D: Mesh(devices.reshape(4, 4, 4), axis_names=('dp', 'fsdp', 'mp'))\n",
    "\n",
    "print(f\"\\n2D Mesh: dp={mesh_2d.shape['dp']}, mp={mesh_2d.shape['mp']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e13fc23",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95a1b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. bfloat16 Mixed Precision ----\n",
    "# TPUs have native bfloat16 support — always use it for training\n",
    "\n",
    "class LinearBF16(nn.Module):\n",
    "    features: int\n",
    "    dtype: jnp.dtype = jnp.bfloat16\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # Compute in bfloat16, keep master weights in float32\n",
    "        kernel = self.param(\n",
    "            'kernel',\n",
    "            nn.initializers.lecun_normal(),\n",
    "            (x.shape[-1], self.features),\n",
    "        )\n",
    "        # Cast to bf16 for matmul (master weights stay fp32)\n",
    "        return x.astype(self.dtype) @ kernel.astype(self.dtype)\n",
    "\n",
    "\n",
    "class TransformerBlockTPU(nn.Module):\n",
    "    \"\"\"Transformer block optimized for TPU with bf16.\"\"\"\n",
    "    num_heads: int\n",
    "    head_dim: int\n",
    "    mlp_dim: int\n",
    "    dtype: jnp.dtype = jnp.bfloat16\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        embed_dim = x.shape[-1]\n",
    "\n",
    "        # Attention\n",
    "        residual = x\n",
    "        x = nn.RMSNorm(dtype=self.dtype)(x)\n",
    "        x = x.astype(self.dtype)\n",
    "\n",
    "        qkv = nn.Dense(3 * self.num_heads * self.head_dim,\n",
    "                        use_bias=False, dtype=self.dtype)(x)\n",
    "        batch, seq_len = x.shape[:2]\n",
    "        qkv = qkv.reshape(batch, seq_len, 3, self.num_heads, self.head_dim)\n",
    "        q, k, v = qkv[:, :, 0], qkv[:, :, 1], qkv[:, :, 2]\n",
    "\n",
    "        scale = 1.0 / jnp.sqrt(self.head_dim).astype(self.dtype)\n",
    "        scores = jnp.einsum('bqhd,bkhd->bhqk', q, k) * scale\n",
    "        mask = jnp.tril(jnp.ones((seq_len, seq_len), dtype=jnp.bool_))[None, None]\n",
    "        scores = jnp.where(mask, scores, jnp.finfo(self.dtype).min)\n",
    "        weights = jax.nn.softmax(scores.astype(jnp.float32), axis=-1).astype(self.dtype)\n",
    "        attn_out = jnp.einsum('bhqk,bkhd->bqhd', weights, v)\n",
    "        attn_out = attn_out.reshape(batch, seq_len, -1)\n",
    "        attn_out = nn.Dense(embed_dim, use_bias=False, dtype=self.dtype)(attn_out)\n",
    "        x = residual + attn_out.astype(x.dtype)\n",
    "\n",
    "        # MLP with SwiGLU\n",
    "        residual = x\n",
    "        x = nn.RMSNorm(dtype=self.dtype)(x)\n",
    "        x = x.astype(self.dtype)\n",
    "        gate = nn.Dense(self.mlp_dim, use_bias=False, dtype=self.dtype)(x)\n",
    "        up = nn.Dense(self.mlp_dim, use_bias=False, dtype=self.dtype)(x)\n",
    "        x = jax.nn.silu(gate) * up\n",
    "        x = nn.Dense(embed_dim, use_bias=False, dtype=self.dtype)(x)\n",
    "        x = residual + x.astype(residual.dtype)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cdb3dd",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703cc862",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Data Parallel Training on TPU ----\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    vocab_size: int = 1024\n",
    "    embed_dim: int = 256\n",
    "    num_heads: int = 4\n",
    "    head_dim: int = 64\n",
    "    mlp_dim: int = 512\n",
    "    num_layers: int = 2\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, tokens):\n",
    "        x = nn.Embed(self.vocab_size, self.embed_dim)(tokens)\n",
    "        for _ in range(self.num_layers):\n",
    "            x = TransformerBlockTPU(\n",
    "                self.num_heads, self.head_dim, self.mlp_dim\n",
    "            )(x)\n",
    "        x = nn.RMSNorm()(x)\n",
    "        return nn.Dense(self.vocab_size, use_bias=False)(x)\n",
    "\n",
    "model = SimpleModel()\n",
    "key = jax.random.PRNGKey(0)\n",
    "dummy = jnp.ones((2, 64), dtype=jnp.int32)\n",
    "params = model.init(key, dummy)['params']\n",
    "\n",
    "num_params = sum(p.size for p in jax.tree.leaves(params))\n",
    "print(f\"\\nModel parameters: {num_params:,}\")\n",
    "\n",
    "# Shard parameters and data across the mesh\n",
    "with mesh_dp:\n",
    "    # Replicate params across all devices\n",
    "    param_sharding = NamedSharding(mesh_dp, P())\n",
    "    params = jax.device_put(params, param_sharding)\n",
    "\n",
    "    # Shard data batch dimension across devices\n",
    "    data_sharding = NamedSharding(mesh_dp, P('dp'))\n",
    "\n",
    "    optimizer = optax.adamw(3e-4, weight_decay=0.1)\n",
    "    opt_state = optimizer.init(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1ee98e",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c464085",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Training with donate_buffers ----\n",
    "# donate_buffers tells XLA it can reuse input buffers for outputs,\n",
    "# reducing TPU HBM memory usage\n",
    "\n",
    "def loss_fn(params, tokens):\n",
    "    logits = model.apply({'params': params}, tokens[:, :-1])\n",
    "    targets = tokens[:, 1:]\n",
    "    log_probs = jax.nn.log_softmax(logits, axis=-1)\n",
    "    target_log_probs = jnp.take_along_axis(\n",
    "        log_probs, targets[:, :, None], axis=-1\n",
    "    )[:, :, 0]\n",
    "    return -jnp.mean(target_log_probs)\n",
    "\n",
    "@partial(jax.jit, donate_argnames=('params', 'opt_state'))\n",
    "def train_step(params, opt_state, tokens):\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(params, tokens)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss\n",
    "\n",
    "print(\"\\n=== Data Parallel Training ===\")\n",
    "with mesh_dp:\n",
    "    for step in range(100):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        # Shard data across devices\n",
    "        tokens = jax.random.randint(subkey, (8, 64), 0, 1024)\n",
    "        tokens = jax.device_put(tokens, data_sharding)\n",
    "\n",
    "        params, opt_state, loss = train_step(params, opt_state, tokens)\n",
    "\n",
    "        if step % 20 == 0:\n",
    "            print(f\"Step {step}: loss={loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9d149e",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6e0de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. FSDP (Fully Sharded Data Parallel) ----\n",
    "# Shard parameters across devices to reduce per-device memory\n",
    "\n",
    "print(\"\\n=== FSDP Training ===\")\n",
    "\n",
    "# Re-initialize for FSDP\n",
    "params_fsdp = model.init(key, dummy)['params']\n",
    "\n",
    "with mesh_dp:\n",
    "    # Shard parameters across data parallel axis\n",
    "    # Each device holds 1/N of the parameters\n",
    "    def shard_params(param):\n",
    "        \"\"\"Shard large params, replicate small ones.\"\"\"\n",
    "        if param.ndim >= 2 and param.shape[0] >= jax.device_count():\n",
    "            return NamedSharding(mesh_dp, P('dp', None))\n",
    "        return NamedSharding(mesh_dp, P())  # replicate small params\n",
    "\n",
    "    param_shardings = jax.tree.map(shard_params, params_fsdp)\n",
    "    params_fsdp = jax.tree.map(\n",
    "        lambda p, s: jax.device_put(p, s),\n",
    "        params_fsdp, param_shardings\n",
    "    )\n",
    "\n",
    "    opt_state_fsdp = optimizer.init(params_fsdp)\n",
    "\n",
    "    # FSDP train step — jit handles the all-gather/reduce-scatter\n",
    "    @jax.jit\n",
    "    def fsdp_train_step(params, opt_state, tokens):\n",
    "        loss, grads = jax.value_and_grad(loss_fn)(params, tokens)\n",
    "        updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        return params, opt_state, loss\n",
    "\n",
    "    for step in range(50):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        tokens = jax.device_put(\n",
    "            jax.random.randint(subkey, (8, 64), 0, 1024),\n",
    "            data_sharding\n",
    "        )\n",
    "        params_fsdp, opt_state_fsdp, loss = fsdp_train_step(\n",
    "            params_fsdp, opt_state_fsdp, tokens\n",
    "        )\n",
    "        if step % 10 == 0:\n",
    "            print(f\"FSDP Step {step}: loss={loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2909e03a",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14e9e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. Tensor Parallelism with 2D Mesh ----\n",
    "\n",
    "print(\"\\n=== 2D Mesh (Data + Tensor Parallel) ===\")\n",
    "\n",
    "with mesh_2d:\n",
    "    # Data sharded on dp axis, model weights sharded on mp axis\n",
    "    data_sharding_2d = NamedSharding(mesh_2d, P('dp'))\n",
    "\n",
    "    # Shard weight matrices along their output dimension\n",
    "    def shard_for_tp(param):\n",
    "        if param.ndim == 2:\n",
    "            return NamedSharding(mesh_2d, P(None, 'mp'))\n",
    "        return NamedSharding(mesh_2d, P())\n",
    "\n",
    "    params_tp = model.init(key, dummy)['params']\n",
    "    tp_shardings = jax.tree.map(shard_for_tp, params_tp)\n",
    "    params_tp = jax.tree.map(\n",
    "        lambda p, s: jax.device_put(p, s), params_tp, tp_shardings\n",
    "    )\n",
    "\n",
    "    print(f\"Mesh shape: dp={mesh_2d.shape['dp']}, mp={mesh_2d.shape['mp']}\")\n",
    "\n",
    "    @jax.jit\n",
    "    def tp_train_step(params, tokens):\n",
    "        loss, grads = jax.value_and_grad(loss_fn)(params, tokens)\n",
    "        return grads, loss\n",
    "\n",
    "    tokens = jax.device_put(\n",
    "        jax.random.randint(key, (4, 64), 0, 1024),\n",
    "        data_sharding_2d\n",
    "    )\n",
    "    grads, loss = tp_train_step(params_tp, tokens)\n",
    "    print(f\"TP forward+backward loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55bcc3b",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a966c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. TPU Performance Tips ----\n",
    "\n",
    "print(\"\\n=== TPU Performance Tips ===\")\n",
    "print(\"\"\"\n",
    "1. Use bfloat16: Native on TPU, 2x throughput vs float32\n",
    "   x = x.astype(jnp.bfloat16)\n",
    "\n",
    "2. Pad batch/sequence to multiples of 128:\n",
    "   TPU tiles are 128x128, so dimensions matching this are fastest\n",
    "\n",
    "3. donate_buffers: Reuse input memory for outputs\n",
    "   @partial(jax.jit, donate_argnames=('params', 'opt_state'))\n",
    "\n",
    "4. Prefetch data: Overlap data loading with computation\n",
    "   Use tf.data or grain for async data pipelines\n",
    "\n",
    "5. Profile with TensorBoard:\n",
    "   jax.profiler.start_trace('/tmp/tensorboard')\n",
    "   # ... training steps ...\n",
    "   jax.profiler.stop_trace()\n",
    "\n",
    "6. Avoid Python overhead in the training loop:\n",
    "   Use jax.lax.scan for the inner loop when possible\n",
    "\n",
    "7. XLA flags for TPU optimization:\n",
    "   XLA_FLAGS='--xla_tpu_megacore_fusion_allow_ags=true'\n",
    "\n",
    "8. Check for recompilation:\n",
    "   jax.config.update('jax_log_compiles', True)\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
