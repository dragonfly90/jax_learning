{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d55e333b",
   "metadata": {},
   "source": [
    "Sharding and Mesh: Modern Device Parallelism in JAX\n",
    "====================================================\n",
    "\n",
    "Key concepts:\n",
    "- Mesh: A logical N-dimensional grid of devices with named axes.\n",
    "- PartitionSpec (P): Describes how array dimensions map to mesh axes.\n",
    "- NamedSharding: Pairs a Mesh with a PartitionSpec.\n",
    "- jax.jit automatically handles distributed computation when inputs are sharded.\n",
    "- This is the recommended approach (over pmap) for new code.\n",
    "\n",
    "Note: Run with multiple simulated devices:\n",
    "  XLA_FLAGS='--xla_force_host_platform_device_count=8' python 05_sharding_and_mesh.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02e1c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.sharding import Mesh, PartitionSpec as P, NamedSharding\n",
    "import os\n",
    "\n",
    "os.environ['XLA_FLAGS'] = '--xla_force_host_platform_device_count=8'\n",
    "\n",
    "devices = jax.devices()\n",
    "print(f\"Devices: {len(devices)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b58c34",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8769ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Creating a Mesh ----\n",
    "# A Mesh is a logical grid of devices with named axes\n",
    "\n",
    "# 1D mesh: 8 devices along axis 'data'\n",
    "mesh_1d = Mesh(jax.devices(), axis_names=('data',))\n",
    "print(f\"1D Mesh: {mesh_1d}\")\n",
    "\n",
    "# 2D mesh: 4x2 grid with axes 'data' and 'model'\n",
    "import numpy as np\n",
    "devices_array = np.array(jax.devices()).reshape(4, 2)\n",
    "mesh_2d = Mesh(devices_array, axis_names=('data', 'model'))\n",
    "print(f\"2D Mesh shape: {mesh_2d.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939fe3e2",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea531e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. PartitionSpec and NamedSharding ----\n",
    "# PartitionSpec maps array dimensions to mesh axes\n",
    "\n",
    "# P('data', None) means:\n",
    "#   - dim 0 is sharded across 'data' axis\n",
    "#   - dim 1 is replicated (not sharded)\n",
    "\n",
    "with mesh_1d:\n",
    "    # Shard a matrix: rows distributed across devices, columns replicated\n",
    "    sharding_rows = NamedSharding(mesh_1d, P('data', None))\n",
    "\n",
    "    # Fully replicated (every device has full copy)\n",
    "    sharding_replicated = NamedSharding(mesh_1d, P(None, None))\n",
    "\n",
    "    # Shard columns instead of rows\n",
    "    sharding_cols = NamedSharding(mesh_1d, P(None, 'data'))\n",
    "\n",
    "# Create a sharded array\n",
    "x = jnp.arange(32).reshape(8, 4)\n",
    "x_sharded = jax.device_put(x, sharding_rows)\n",
    "print(f\"\\nSharded array shape: {x_sharded.shape}\")\n",
    "print(f\"Sharding: {x_sharded.sharding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c86b804",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53749898",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Automatic Parallelism with jit ----\n",
    "# When inputs are sharded, jit automatically parallelizes the computation\n",
    "\n",
    "@jax.jit\n",
    "def matmul(x, y):\n",
    "    return x @ y\n",
    "\n",
    "with mesh_1d:\n",
    "    # x: sharded by rows, y: replicated\n",
    "    x = jax.device_put(jnp.ones((8, 4)), NamedSharding(mesh_1d, P('data', None)))\n",
    "    y = jax.device_put(jnp.ones((4, 6)), NamedSharding(mesh_1d, P(None, None)))\n",
    "\n",
    "    result = matmul(x, y)\n",
    "    print(f\"\\nMatmul result shape: {result.shape}\")\n",
    "    print(f\"Result sharding: {result.sharding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0551996",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae25f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Specifying Output Sharding ----\n",
    "# Use jax.jit's out_shardings to control output layout\n",
    "\n",
    "with mesh_1d:\n",
    "    out_sharding = NamedSharding(mesh_1d, P('data', None))\n",
    "\n",
    "    @jax.jit\n",
    "    def compute(x):\n",
    "        return jnp.sin(x) + jnp.cos(x)\n",
    "\n",
    "    x = jax.device_put(jnp.ones((8, 4)), NamedSharding(mesh_1d, P('data', None)))\n",
    "    result = compute(x)\n",
    "    print(f\"\\nOutput sharding: {result.sharding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bbbd2b",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6affccab",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. 2D Mesh: Data + Model Parallelism ----\n",
    "# Common in large model training: shard data AND model weights\n",
    "\n",
    "with mesh_2d:\n",
    "    # Data: shard batch dim across 'data', replicate features\n",
    "    data_sharding = NamedSharding(mesh_2d, P('data', None))\n",
    "\n",
    "    # Weights: shard output dim across 'model', replicate input dim\n",
    "    weight_sharding = NamedSharding(mesh_2d, P(None, 'model'))\n",
    "\n",
    "    # Bias: shard across 'model' axis\n",
    "    bias_sharding = NamedSharding(mesh_2d, P('model',))\n",
    "\n",
    "    @jax.jit\n",
    "    def linear_forward(x, w, b):\n",
    "        return x @ w + b\n",
    "\n",
    "    batch_size = 16\n",
    "    in_features = 8\n",
    "    out_features = 4\n",
    "\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    x = jax.device_put(\n",
    "        jax.random.normal(key, (batch_size, in_features)),\n",
    "        data_sharding\n",
    "    )\n",
    "    w = jax.device_put(\n",
    "        jax.random.normal(key, (in_features, out_features)),\n",
    "        weight_sharding\n",
    "    )\n",
    "    b = jax.device_put(jnp.zeros(out_features), bias_sharding)\n",
    "\n",
    "    output = linear_forward(x, w, b)\n",
    "    print(f\"\\n2D Mesh output shape: {output.shape}\")\n",
    "    print(f\"2D Mesh output sharding: {output.sharding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922869fc",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c2c179",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. FSDP-style Training (Fully Sharded Data Parallel) ----\n",
    "# Shard both data and parameters across devices\n",
    "\n",
    "def loss_fn(params, x, y):\n",
    "    pred = x @ params['w'] + params['b']\n",
    "    return jnp.mean((pred - y) ** 2)\n",
    "\n",
    "with mesh_1d:\n",
    "    param_sharding = NamedSharding(mesh_1d, P(None, None))  # replicate params\n",
    "    data_sharding = NamedSharding(mesh_1d, P('data', None))\n",
    "\n",
    "    params = {\n",
    "        'w': jax.device_put(jax.random.normal(key, (4, 2)), param_sharding),\n",
    "        'b': jax.device_put(jnp.zeros(2), NamedSharding(mesh_1d, P(None))),\n",
    "    }\n",
    "\n",
    "    x = jax.device_put(jax.random.normal(key, (8, 4)), data_sharding)\n",
    "    y = jax.device_put(jax.random.normal(key, (8, 2)), data_sharding)\n",
    "\n",
    "    @jax.jit\n",
    "    def train_step(params, x, y):\n",
    "        loss, grads = jax.value_and_grad(loss_fn)(params, x, y)\n",
    "        new_params = jax.tree.map(lambda p, g: p - 0.01 * g, params, grads)\n",
    "        return new_params, loss\n",
    "\n",
    "    for step in range(5):\n",
    "        params, loss = train_step(params, x, y)\n",
    "        print(f\"Step {step}: loss={loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4828dd46",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf486c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. Visualizing Sharding ----\n",
    "# jax.debug.visualize_array_sharding shows how data is distributed\n",
    "\n",
    "with mesh_1d:\n",
    "    x = jax.device_put(jnp.ones((8, 4)), NamedSharding(mesh_1d, P('data', None)))\n",
    "    print(\"\\nArray sharding visualization:\")\n",
    "    jax.debug.visualize_array_sharding(x)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
