{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a273a2f2",
   "metadata": {},
   "source": [
    "Distributed Multi-Host Training with JAX\n",
    "==========================================\n",
    "\n",
    "Key concepts:\n",
    "- JAX multi-host: Each host (machine) runs the same Python program.\n",
    "- jax.distributed.initialize() sets up communication between hosts.\n",
    "- Mesh spans all devices across all hosts — sharding is global.\n",
    "- Data loading must be coordinated: each host loads its own shard.\n",
    "- Checkpointing with orbax for distributed saving/loading.\n",
    "\n",
    "This file covers patterns for multi-host TPU pod training.\n",
    "On a single machine, the patterns still work with simulated devices.\n",
    "\n",
    "To run on TPU pod (e.g., v4-32 = 4 hosts x 4 chips):\n",
    "  # On each host:\n",
    "  python 13_distributed_training.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4f65ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.sharding import Mesh, PartitionSpec as P, NamedSharding\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "if 'TPU_CHIPS_PER_HOST_BOUNDS' not in os.environ:\n",
    "    os.environ['XLA_FLAGS'] = '--xla_force_host_platform_device_count=8'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9653134",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d384babd",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Multi-Host Initialization ----\n",
    "# On TPU pods, call this before any JAX operation\n",
    "\n",
    "# jax.distributed.initialize()  # Uncomment on actual multi-host setup\n",
    "# This auto-discovers other hosts via environment variables:\n",
    "#   - MEGASCALE_COORDINATOR_ADDRESS\n",
    "#   - MEGASCALE_NUM_PROCESSES\n",
    "#   - MEGASCALE_PROCESS_ID\n",
    "# Or pass them explicitly:\n",
    "# jax.distributed.initialize(\n",
    "#     coordinator_address=\"host0:1234\",\n",
    "#     num_processes=4,\n",
    "#     process_id=jax.process_index(),\n",
    "# )\n",
    "\n",
    "print(\"=== Distributed Setup ===\")\n",
    "print(f\"Process {jax.process_index()} of {jax.process_count()}\")\n",
    "print(f\"Global devices: {jax.device_count()}\")\n",
    "print(f\"Local devices: {jax.local_device_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93697e03",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364f855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Global Mesh Across All Hosts ----\n",
    "\n",
    "all_devices = jax.devices()\n",
    "print(f\"\\nAll devices: {len(all_devices)}\")\n",
    "\n",
    "# Create a global mesh spanning all hosts\n",
    "# e.g., 32 chips -> (8, 4) for dp=8, mp=4\n",
    "num_devices = len(all_devices)\n",
    "dp_size = num_devices // 2 if num_devices > 1 else 1\n",
    "mp_size = min(2, num_devices)\n",
    "\n",
    "mesh = Mesh(\n",
    "    np.array(all_devices).reshape(dp_size, mp_size),\n",
    "    axis_names=('dp', 'mp')\n",
    ")\n",
    "print(f\"Global mesh: dp={dp_size}, mp={mp_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79653757",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab8b670",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Per-Host Data Loading ----\n",
    "# Each host must load only its shard of the global batch\n",
    "\n",
    "def get_local_batch(global_batch_size, seq_len, vocab_size, key):\n",
    "    \"\"\"\n",
    "    Each host generates/loads only its portion of the global batch.\n",
    "    With data parallelism, host i gets rows [i*local_batch : (i+1)*local_batch].\n",
    "    \"\"\"\n",
    "    num_hosts = jax.process_count()\n",
    "    local_batch_size = global_batch_size // num_hosts\n",
    "    host_id = jax.process_index()\n",
    "\n",
    "    # In real training, you'd use a data pipeline (tf.data, grain)\n",
    "    # that skips to the correct shard:\n",
    "    #   dataset.shard(num_shards=num_hosts, index=host_id)\n",
    "    key = jax.random.fold_in(key, host_id)\n",
    "    local_tokens = jax.random.randint(\n",
    "        key, (local_batch_size, seq_len), 0, vocab_size\n",
    "    )\n",
    "    return local_tokens\n",
    "\n",
    "# Each host creates its local batch\n",
    "key = jax.random.PRNGKey(42)\n",
    "local_batch = get_local_batch(\n",
    "    global_batch_size=32, seq_len=64, vocab_size=1024, key=key\n",
    ")\n",
    "print(f\"\\nLocal batch shape: {local_batch.shape}\")\n",
    "\n",
    "# Place local data on local devices with global sharding spec\n",
    "with mesh:\n",
    "    global_sharding = NamedSharding(mesh, P('dp'))\n",
    "\n",
    "    # make_array_from_single_device_arrays builds a globally-sharded array\n",
    "    # from each host's local data\n",
    "    local_devices = jax.local_devices()\n",
    "    per_device = local_batch.shape[0] // len(local_devices)\n",
    "\n",
    "    local_arrays = [\n",
    "        jax.device_put(local_batch[i * per_device:(i + 1) * per_device], d)\n",
    "        for i, d in enumerate(local_devices)\n",
    "    ]\n",
    "\n",
    "    # In multi-host, this creates a global array from local shards\n",
    "    # global_batch = jax.make_array_from_single_device_arrays(\n",
    "    #     shape=(32, 64),\n",
    "    #     sharding=global_sharding,\n",
    "    #     arrays=local_arrays,\n",
    "    # )\n",
    "    # For single-host demo, just use device_put:\n",
    "    global_batch = jax.device_put(local_batch, global_sharding)\n",
    "    print(f\"Global batch sharding: {global_batch.sharding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80904820",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1361dc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Distributed Training Loop Pattern ----\n",
    "\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "\n",
    "class SmallModel(nn.Module):\n",
    "    vocab_size: int = 1024\n",
    "    dim: int = 128\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, tokens):\n",
    "        x = nn.Embed(self.vocab_size, self.dim)(tokens)\n",
    "        x = nn.Dense(self.dim)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(self.vocab_size)(x)\n",
    "        return x\n",
    "\n",
    "model = SmallModel()\n",
    "params = model.init(key, jnp.ones((1, 64), dtype=jnp.int32))['params']\n",
    "optimizer = optax.adamw(1e-3)\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "with mesh:\n",
    "    # Replicate params and optimizer state\n",
    "    replicated = NamedSharding(mesh, P())\n",
    "    params = jax.device_put(params, replicated)\n",
    "    opt_state = jax.device_put(opt_state, replicated)\n",
    "\n",
    "    def loss_fn(params, tokens):\n",
    "        logits = model.apply({'params': params}, tokens[:, :-1])\n",
    "        targets = tokens[:, 1:]\n",
    "        return -jnp.mean(\n",
    "            jnp.take_along_axis(\n",
    "                jax.nn.log_softmax(logits), targets[:, :, None], axis=-1\n",
    "            )\n",
    "        )\n",
    "\n",
    "    @jax.jit\n",
    "    def train_step(params, opt_state, tokens):\n",
    "        loss, grads = jax.value_and_grad(loss_fn)(params, tokens)\n",
    "        updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        return params, opt_state, loss\n",
    "\n",
    "    data_sharding = NamedSharding(mesh, P('dp'))\n",
    "\n",
    "    print(\"\\n=== Distributed Training ===\")\n",
    "    for step in range(50):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        tokens = jax.device_put(\n",
    "            jax.random.randint(subkey, (8, 64), 0, 1024),\n",
    "            data_sharding\n",
    "        )\n",
    "        params, opt_state, loss = train_step(params, opt_state, tokens)\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            print(f\"Step {step}: loss={loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9901450",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d3259b",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Checkpointing with Orbax ----\n",
    "# orbax handles distributed checkpoint saving/loading\n",
    "\n",
    "print(\"\\n=== Checkpointing ===\")\n",
    "print(\"\"\"\n",
    "# pip install orbax-checkpoint\n",
    "\n",
    "import orbax.checkpoint as ocp\n",
    "\n",
    "# Create a checkpointer\n",
    "checkpointer = ocp.StandardCheckpointer()\n",
    "\n",
    "# Save (each host saves its local shard)\n",
    "checkpointer.save(\n",
    "    '/tmp/checkpoint/step_100',\n",
    "    args=ocp.args.StandardSave(params),\n",
    ")\n",
    "\n",
    "# Restore with target shardings\n",
    "abstract_params = jax.tree.map(\n",
    "    lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype),\n",
    "    params\n",
    ")\n",
    "restored = checkpointer.restore(\n",
    "    '/tmp/checkpoint/step_100',\n",
    "    args=ocp.args.StandardRestore(abstract_params),\n",
    ")\n",
    "\n",
    "# For async checkpointing (non-blocking):\n",
    "async_checkpointer = ocp.AsyncCheckpointer(ocp.StandardCheckpointHandler())\n",
    "async_checkpointer.save(path, args=ocp.args.StandardSave(params))\n",
    "async_checkpointer.wait_until_finished()  # call before program exit\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaeb6a2b",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb0e5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Multi-Host Debugging Tips ----\n",
    "\n",
    "print(\"=== Multi-Host Debugging ===\")\n",
    "print(\"\"\"\n",
    "1. Check all hosts see the same devices:\n",
    "   print(f\"Host {jax.process_index()}: {jax.device_count()} devices\")\n",
    "\n",
    "2. Verify sharding is correct:\n",
    "   jax.debug.visualize_array_sharding(array)\n",
    "\n",
    "3. Log only from host 0:\n",
    "   if jax.process_index() == 0:\n",
    "       print(f\"Loss: {loss}\")\n",
    "\n",
    "4. Barrier synchronization:\n",
    "   # Ensure all hosts reach the same point\n",
    "   jax.experimental.multihost_utils.sync_global_devices(\"checkpoint\")\n",
    "\n",
    "5. Common errors:\n",
    "   - \"Mismatched mesh\" — all hosts must create identical Mesh objects\n",
    "   - Hanging — usually one host crashed; check all host logs\n",
    "   - Shape mismatch — verify per-host data shape matches sharding\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
