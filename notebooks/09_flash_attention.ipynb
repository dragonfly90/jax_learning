{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc89363d",
   "metadata": {},
   "source": [
    "Flash Attention in JAX\n",
    "=======================\n",
    "\n",
    "Key concepts:\n",
    "- Flash Attention computes exact attention without materializing the full\n",
    "  N×N attention matrix, reducing memory from O(N²) to O(N).\n",
    "- It works by tiling Q, K, V into blocks and computing attention incrementally\n",
    "  using the online softmax trick (log-sum-exp correction).\n",
    "- JAX's splash_attention / dot_product_attention provides hardware-optimized\n",
    "  implementations on TPU; this file shows both the manual algorithm and\n",
    "  the built-in API.\n",
    "\n",
    "Reference: \"FlashAttention: Fast and Memory-Efficient Exact Attention\n",
    "with IO-Awareness\" (Dao et al., 2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f853b8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19eb198a",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c83c1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Standard (Naive) Attention ----\n",
    "# Materializes the full N×N matrix — O(N²) memory\n",
    "\n",
    "def naive_attention(q, k, v, mask=None):\n",
    "    \"\"\"\n",
    "    Standard scaled dot-product attention.\n",
    "    q, k, v: (batch, heads, seq_len, head_dim)\n",
    "    \"\"\"\n",
    "    d_k = q.shape[-1]\n",
    "    scores = jnp.matmul(q, k.transpose(0, 1, 3, 2)) / jnp.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = jnp.where(mask, scores, -1e9)\n",
    "    weights = jax.nn.softmax(scores, axis=-1)\n",
    "    return jnp.matmul(weights, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afc08c2",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe19a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Flash Attention (Manual Block-wise Implementation) ----\n",
    "# Processes Q in blocks, iterates over K/V blocks, never stores full N×N\n",
    "\n",
    "def flash_attention_manual(q, k, v, block_size=64):\n",
    "    \"\"\"\n",
    "    Flash Attention via block-wise computation with online softmax.\n",
    "    q, k, v: (seq_len, head_dim) — single head, no batch for clarity.\n",
    "\n",
    "    Algorithm:\n",
    "    1. Split Q into blocks of size Br\n",
    "    2. For each Q block, iterate over all K/V blocks\n",
    "    3. Compute local attention scores and accumulate output using\n",
    "       the online log-sum-exp trick to avoid materializing full NxN\n",
    "    \"\"\"\n",
    "    seq_len, d = q.shape\n",
    "    num_blocks_q = seq_len // block_size\n",
    "    num_blocks_kv = seq_len // block_size\n",
    "    scale = 1.0 / jnp.sqrt(d)\n",
    "\n",
    "    output = jnp.zeros_like(q)\n",
    "\n",
    "    for i in range(num_blocks_q):\n",
    "        q_block = q[i * block_size:(i + 1) * block_size]  # (Br, d)\n",
    "\n",
    "        # Running statistics for online softmax\n",
    "        m_i = jnp.full((block_size, 1), -jnp.inf)  # row-wise max\n",
    "        l_i = jnp.zeros((block_size, 1))             # row-wise sum of exp\n",
    "        o_i = jnp.zeros((block_size, d))              # accumulated output\n",
    "\n",
    "        for j in range(num_blocks_kv):\n",
    "            k_block = k[j * block_size:(j + 1) * block_size]  # (Bc, d)\n",
    "            v_block = v[j * block_size:(j + 1) * block_size]  # (Bc, d)\n",
    "\n",
    "            # Local attention scores: (Br, Bc)\n",
    "            s_ij = (q_block @ k_block.T) * scale\n",
    "\n",
    "            # Online softmax update\n",
    "            m_ij = jnp.max(s_ij, axis=-1, keepdims=True)       # (Br, 1)\n",
    "            m_new = jnp.maximum(m_i, m_ij)                       # (Br, 1)\n",
    "\n",
    "            # Correction factors for previously accumulated values\n",
    "            exp_old = jnp.exp(m_i - m_new)\n",
    "            exp_new = jnp.exp(m_ij - m_new)\n",
    "\n",
    "            p_ij = jnp.exp(s_ij - m_new)  # (Br, Bc) softmax numerator\n",
    "\n",
    "            l_new = exp_old * l_i + jnp.sum(p_ij, axis=-1, keepdims=True)\n",
    "\n",
    "            # Update output: rescale old output + add new contribution\n",
    "            o_i = exp_old * o_i + p_ij @ v_block\n",
    "\n",
    "            m_i = m_new\n",
    "            l_i = l_new\n",
    "\n",
    "        # Normalize\n",
    "        o_i = o_i / l_i\n",
    "        output = output.at[i * block_size:(i + 1) * block_size].set(o_i)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e8243c",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c634f207",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Verify correctness ----\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "seq_len, d = 256, 64\n",
    "\n",
    "q = jax.random.normal(key, (seq_len, d))\n",
    "k = jax.random.normal(key, (seq_len, d))\n",
    "v = jax.random.normal(key, (seq_len, d))\n",
    "\n",
    "naive_out = naive_attention(\n",
    "    q[None, None], k[None, None], v[None, None]\n",
    ")[0, 0]\n",
    "flash_out = flash_attention_manual(q, k, v, block_size=64)\n",
    "\n",
    "print(f\"Max difference (naive vs flash): {jnp.max(jnp.abs(naive_out - flash_out)):.2e}\")\n",
    "print(f\"Mean difference: {jnp.mean(jnp.abs(naive_out - flash_out)):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c9b9a6",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c86e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. JIT-compiled batched Flash Attention ----\n",
    "\n",
    "@partial(jax.jit, static_argnames=['block_size'])\n",
    "def flash_attention_jit(q, k, v, block_size=64):\n",
    "    \"\"\"\n",
    "    JIT-friendly flash attention using lax.scan over KV blocks.\n",
    "    q, k, v: (seq_len, head_dim)\n",
    "    \"\"\"\n",
    "    seq_len, d = q.shape\n",
    "    scale = 1.0 / jnp.sqrt(d)\n",
    "    num_blocks_kv = seq_len // block_size\n",
    "\n",
    "    # Reshape K, V into blocks: (num_blocks, block_size, d)\n",
    "    k_blocks = k.reshape(num_blocks_kv, block_size, d)\n",
    "    v_blocks = v.reshape(num_blocks_kv, block_size, d)\n",
    "\n",
    "    def process_query_block(q_block):\n",
    "        \"\"\"Process one Q block against all KV blocks.\"\"\"\n",
    "        def scan_fn(carry, kv_block):\n",
    "            m_i, l_i, o_i = carry\n",
    "            k_block, v_block = kv_block\n",
    "\n",
    "            s_ij = (q_block @ k_block.T) * scale\n",
    "            m_ij = jnp.max(s_ij, axis=-1, keepdims=True)\n",
    "            m_new = jnp.maximum(m_i, m_ij)\n",
    "\n",
    "            exp_old = jnp.exp(m_i - m_new)\n",
    "            p_ij = jnp.exp(s_ij - m_new)\n",
    "            l_new = exp_old * l_i + jnp.sum(p_ij, axis=-1, keepdims=True)\n",
    "            o_i = exp_old * o_i + p_ij @ v_block\n",
    "\n",
    "            return (m_new, l_new, o_i), None\n",
    "\n",
    "        init = (\n",
    "            jnp.full((block_size, 1), -jnp.inf),\n",
    "            jnp.zeros((block_size, 1)),\n",
    "            jnp.zeros((block_size, d)),\n",
    "        )\n",
    "        (m_final, l_final, o_final), _ = jax.lax.scan(\n",
    "            scan_fn, init, (k_blocks, v_blocks)\n",
    "        )\n",
    "        return o_final / l_final\n",
    "\n",
    "    # Process all Q blocks via vmap\n",
    "    q_blocks = q.reshape(-1, block_size, d)\n",
    "    output_blocks = jax.vmap(process_query_block)(q_blocks)\n",
    "    return output_blocks.reshape(seq_len, d)\n",
    "\n",
    "flash_jit_out = flash_attention_jit(q, k, v)\n",
    "print(f\"\\nJIT flash vs naive max diff: {jnp.max(jnp.abs(naive_out - flash_jit_out)):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0446b024",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94a53dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Multi-head Flash Attention ----\n",
    "\n",
    "@partial(jax.jit, static_argnames=['block_size'])\n",
    "def multi_head_flash_attention(q, k, v, block_size=64):\n",
    "    \"\"\"\n",
    "    Multi-head flash attention.\n",
    "    q, k, v: (batch, num_heads, seq_len, head_dim)\n",
    "    \"\"\"\n",
    "    def single_head_flash(q, k, v):\n",
    "        return flash_attention_jit(q, k, v, block_size=block_size)\n",
    "\n",
    "    # vmap over heads, then over batch\n",
    "    batched = jax.vmap(jax.vmap(single_head_flash))(q, k, v)\n",
    "    return batched\n",
    "\n",
    "batch, heads, seq_len, d = 2, 8, 256, 64\n",
    "key1, key2, key3 = jax.random.split(key, 3)\n",
    "Q = jax.random.normal(key1, (batch, heads, seq_len, d))\n",
    "K = jax.random.normal(key2, (batch, heads, seq_len, d))\n",
    "V = jax.random.normal(key3, (batch, heads, seq_len, d))\n",
    "\n",
    "mha_out = multi_head_flash_attention(Q, K, V)\n",
    "naive_mha = naive_attention(Q, K, V)\n",
    "print(f\"\\nMulti-head flash attention output shape: {mha_out.shape}\")\n",
    "print(f\"MHA max diff vs naive: {jnp.max(jnp.abs(mha_out - naive_mha)):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06caf78",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736179f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Causal (Autoregressive) Flash Attention ----\n",
    "\n",
    "@partial(jax.jit, static_argnames=['block_size'])\n",
    "def causal_flash_attention(q, k, v, block_size=64):\n",
    "    \"\"\"\n",
    "    Causal flash attention — each position only attends to previous positions.\n",
    "    q, k, v: (seq_len, head_dim)\n",
    "    \"\"\"\n",
    "    seq_len, d = q.shape\n",
    "    scale = 1.0 / jnp.sqrt(d)\n",
    "    num_blocks = seq_len // block_size\n",
    "\n",
    "    k_blocks = k.reshape(num_blocks, block_size, d)\n",
    "    v_blocks = v.reshape(num_blocks, block_size, d)\n",
    "\n",
    "    def process_query_block(q_idx_and_block):\n",
    "        q_idx, q_block = q_idx_and_block\n",
    "\n",
    "        def scan_fn(carry, kv_idx_and_block):\n",
    "            m_i, l_i, o_i = carry\n",
    "            kv_idx, k_block, v_block = kv_idx_and_block\n",
    "\n",
    "            s_ij = (q_block @ k_block.T) * scale\n",
    "\n",
    "            # Causal mask: Q position i can attend to K position j if j <= i\n",
    "            q_positions = q_idx * block_size + jnp.arange(block_size)[:, None]\n",
    "            k_positions = kv_idx * block_size + jnp.arange(block_size)[None, :]\n",
    "            causal_mask = q_positions >= k_positions\n",
    "            s_ij = jnp.where(causal_mask, s_ij, -1e9)\n",
    "\n",
    "            m_ij = jnp.max(s_ij, axis=-1, keepdims=True)\n",
    "            m_new = jnp.maximum(m_i, m_ij)\n",
    "            exp_old = jnp.exp(m_i - m_new)\n",
    "            p_ij = jnp.exp(s_ij - m_new)\n",
    "            l_new = exp_old * l_i + jnp.sum(p_ij, axis=-1, keepdims=True)\n",
    "            o_i = exp_old * o_i + p_ij @ v_block\n",
    "\n",
    "            return (m_new, l_new, o_i), None\n",
    "\n",
    "        init = (\n",
    "            jnp.full((block_size, 1), -jnp.inf),\n",
    "            jnp.zeros((block_size, 1)),\n",
    "            jnp.zeros((block_size, d)),\n",
    "        )\n",
    "        kv_indices = jnp.arange(num_blocks)\n",
    "        (m_f, l_f, o_f), _ = jax.lax.scan(\n",
    "            scan_fn, init, (kv_indices, k_blocks, v_blocks)\n",
    "        )\n",
    "        return o_f / l_f\n",
    "\n",
    "    q_blocks = q.reshape(num_blocks, block_size, d)\n",
    "    q_indices = jnp.arange(num_blocks)\n",
    "    output_blocks = jax.vmap(process_query_block)((q_indices, q_blocks))\n",
    "    return output_blocks.reshape(seq_len, d)\n",
    "\n",
    "# Verify causal attention\n",
    "q_test = jax.random.normal(key, (256, 64))\n",
    "k_test = jax.random.normal(key, (256, 64))\n",
    "v_test = jax.random.normal(key, (256, 64))\n",
    "\n",
    "causal_out = causal_flash_attention(q_test, k_test, v_test)\n",
    "\n",
    "# Compare with naive causal\n",
    "causal_mask = jnp.tril(jnp.ones((256, 256), dtype=bool))[None, None]\n",
    "naive_causal = naive_attention(\n",
    "    q_test[None, None], k_test[None, None], v_test[None, None], mask=causal_mask\n",
    ")[0, 0]\n",
    "print(f\"\\nCausal flash vs naive max diff: {jnp.max(jnp.abs(causal_out - naive_causal)):.2e}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
