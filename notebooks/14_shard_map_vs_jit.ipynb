{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b14f3826",
   "metadata": {},
   "source": [
    "shard_map vs jax.jit: Explicit vs Automatic Parallelism\n",
    "========================================================\n",
    "\n",
    "Key concepts:\n",
    "- shard_map: Low-level API — you write per-shard code and handle collectives\n",
    "  (psum, pall_to_all, etc.) explicitly. Full control over communication.\n",
    "- jax.jit + NamedSharding: High-level API — you shard the data, JIT figures\n",
    "  out the communication automatically via the XLA compiler.\n",
    "- Both use Mesh and PartitionSpec to describe device layout.\n",
    "- shard_map is useful for custom communication patterns (e.g., ring allreduce,\n",
    "  pipeline parallelism). jax.jit is preferred for most standard patterns.\n",
    "\n",
    "Note: Run with multiple simulated devices:\n",
    "  XLA_FLAGS='--xla_force_host_platform_device_count=8' python 14_shard_map_vs_jit.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8163a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.sharding import Mesh, PartitionSpec as P, NamedSharding\n",
    "from jax.experimental.shard_map import shard_map\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ['XLA_FLAGS'] = '--xla_force_host_platform_device_count=8'\n",
    "\n",
    "devices = jax.devices()\n",
    "mesh = Mesh(np.array(devices), axis_names=('dp',))\n",
    "print(f\"Devices: {len(devices)}, Mesh: {mesh.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8582121c",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30edc8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Basic Example: Element-wise Operation ----\n",
    "# The simplest case — no communication needed between devices.\n",
    "\n",
    "print(\"\\n=== 1. Element-wise Operation ===\")\n",
    "\n",
    "x = jnp.arange(32.0).reshape(8, 4)  # 8 rows, 4 cols\n",
    "\n",
    "# --- shard_map version ---\n",
    "# The function receives a LOCAL shard (1 row per device on 8 devices)\n",
    "@shard_map(mesh, in_specs=(P('dp', None),), out_specs=P('dp', None))\n",
    "def elementwise_shard(x_shard):\n",
    "    # x_shard shape: (1, 4) — each device sees only its local piece\n",
    "    return jnp.sin(x_shard) + jnp.cos(x_shard)\n",
    "\n",
    "result_shard = elementwise_shard(x)\n",
    "print(f\"shard_map result shape: {result_shard.shape}\")\n",
    "\n",
    "# --- jax.jit version ---\n",
    "# Same function, but JIT handles sharding automatically\n",
    "@jax.jit\n",
    "def elementwise_jit(x):\n",
    "    # x shape: (8, 4) — you write global-view code\n",
    "    return jnp.sin(x) + jnp.cos(x)\n",
    "\n",
    "x_sharded = jax.device_put(x, NamedSharding(mesh, P('dp', None)))\n",
    "result_jit = elementwise_jit(x_sharded)\n",
    "print(f\"jax.jit  result shape: {result_jit.shape}\")\n",
    "print(f\"Results match: {jnp.allclose(result_shard, result_jit)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee32eaa",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a80051",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. AllReduce: Sum Across Devices ----\n",
    "# This is where the difference matters — shard_map requires explicit psum.\n",
    "\n",
    "print(\"\\n=== 2. AllReduce (Sum Across Devices) ===\")\n",
    "\n",
    "x = jax.random.normal(jax.random.PRNGKey(0), (8, 4))\n",
    "\n",
    "# --- shard_map version ---\n",
    "# You must call lax.psum explicitly to sum across devices\n",
    "@shard_map(mesh, in_specs=(P('dp', None),), out_specs=P(None, None))\n",
    "def mean_shard(x_shard):\n",
    "    # x_shard: (1, 4) per device\n",
    "    local_sum = jnp.sum(x_shard, axis=0, keepdims=True)  # (1, 4)\n",
    "    # Explicit allreduce: sum across all devices in the 'dp' axis\n",
    "    global_sum = jax.lax.psum(local_sum, axis_name='dp')  # (1, 4)\n",
    "    return global_sum / 8.0  # manual mean\n",
    "\n",
    "result_shard = mean_shard(x)\n",
    "print(f\"shard_map mean: {result_shard[0, :3]}\")\n",
    "\n",
    "# --- jax.jit version ---\n",
    "# Just write jnp.mean — JIT inserts the allreduce automatically\n",
    "@jax.jit\n",
    "def mean_jit(x):\n",
    "    return jnp.mean(x, axis=0, keepdims=True)\n",
    "\n",
    "x_sharded = jax.device_put(x, NamedSharding(mesh, P('dp', None)))\n",
    "result_jit = mean_jit(x_sharded)\n",
    "print(f\"jax.jit  mean: {result_jit[0, :3]}\")\n",
    "print(f\"Results match: {jnp.allclose(result_shard, result_jit)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f8267c",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8418d739",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Data-Parallel Gradient Computation ----\n",
    "# The most common use case: compute gradients on data shards, then allreduce.\n",
    "\n",
    "print(\"\\n=== 3. Data-Parallel Gradient Computation ===\")\n",
    "\n",
    "# Simple linear model: y = x @ w + b\n",
    "key = jax.random.PRNGKey(42)\n",
    "w = jax.random.normal(key, (4, 2))\n",
    "b = jnp.zeros(2)\n",
    "x_data = jax.random.normal(key, (8, 4))\n",
    "y_data = x_data @ w + 0.1 * jax.random.normal(key, (8, 2))\n",
    "\n",
    "def loss_fn(w, b, x, y):\n",
    "    pred = x @ w + b\n",
    "    return jnp.mean((pred - y) ** 2)\n",
    "\n",
    "# --- shard_map version ---\n",
    "# Each device computes local gradients, then we allreduce\n",
    "@shard_map(\n",
    "    mesh,\n",
    "    in_specs=(P(None, None), P(None,), P('dp', None), P('dp', None)),\n",
    "    out_specs=(P(None, None), P(None,), P()),\n",
    ")\n",
    "def grad_step_shard(w, b, x_shard, y_shard):\n",
    "    # Each device has 1/8 of the data\n",
    "    local_loss, (dw, db) = jax.value_and_grad(loss_fn, argnums=(0, 1))(\n",
    "        w, b, x_shard, y_shard\n",
    "    )\n",
    "    # Allreduce: average gradients across all devices\n",
    "    dw = jax.lax.pmean(dw, axis_name='dp')\n",
    "    db = jax.lax.pmean(db, axis_name='dp')\n",
    "    loss = jax.lax.pmean(local_loss, axis_name='dp')\n",
    "    return dw, db, loss\n",
    "\n",
    "dw_shard, db_shard, loss_shard = grad_step_shard(w, b, x_data, y_data)\n",
    "print(f\"shard_map — loss: {loss_shard:.4f}, grad_w norm: {jnp.linalg.norm(dw_shard):.4f}\")\n",
    "\n",
    "# --- jax.jit version ---\n",
    "# Same result, but JIT handles everything\n",
    "@jax.jit\n",
    "def grad_step_jit(w, b, x, y):\n",
    "    loss, (dw, db) = jax.value_and_grad(loss_fn, argnums=(0, 1))(w, b, x, y)\n",
    "    return dw, db, loss\n",
    "\n",
    "w_s = jax.device_put(w, NamedSharding(mesh, P(None, None)))\n",
    "b_s = jax.device_put(b, NamedSharding(mesh, P(None,)))\n",
    "x_s = jax.device_put(x_data, NamedSharding(mesh, P('dp', None)))\n",
    "y_s = jax.device_put(y_data, NamedSharding(mesh, P('dp', None)))\n",
    "\n",
    "dw_jit, db_jit, loss_jit = grad_step_jit(w_s, b_s, x_s, y_s)\n",
    "print(f\"jax.jit  — loss: {loss_jit:.4f}, grad_w norm: {jnp.linalg.norm(dw_jit):.4f}\")\n",
    "print(f\"Results match: {jnp.allclose(dw_shard, dw_jit, atol=1e-5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c68da9d",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ca7dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Full Training Loop Comparison ----\n",
    "# Side-by-side training loops using both approaches.\n",
    "\n",
    "print(\"\\n=== 4. Training Loop — shard_map ===\")\n",
    "\n",
    "w_sm = w.copy()\n",
    "b_sm = b.copy()\n",
    "lr = 0.01\n",
    "\n",
    "@shard_map(\n",
    "    mesh,\n",
    "    in_specs=(P(None, None), P(None,), P('dp', None), P('dp', None)),\n",
    "    out_specs=(P(None, None), P(None,), P()),\n",
    ")\n",
    "def train_step_shard(w, b, x_shard, y_shard):\n",
    "    loss, (dw, db) = jax.value_and_grad(loss_fn, argnums=(0, 1))(\n",
    "        w, b, x_shard, y_shard\n",
    "    )\n",
    "    dw = jax.lax.pmean(dw, axis_name='dp')\n",
    "    db = jax.lax.pmean(db, axis_name='dp')\n",
    "    loss = jax.lax.pmean(loss, axis_name='dp')\n",
    "    w = w - lr * dw\n",
    "    b = b - lr * db\n",
    "    return w, b, loss\n",
    "\n",
    "for step in range(5):\n",
    "    w_sm, b_sm, loss_val = train_step_shard(w_sm, b_sm, x_data, y_data)\n",
    "    print(f\"  Step {step}: loss={loss_val:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\n=== 4. Training Loop — jax.jit ===\")\n",
    "\n",
    "w_jt = jax.device_put(w.copy(), NamedSharding(mesh, P(None, None)))\n",
    "b_jt = jax.device_put(b.copy(), NamedSharding(mesh, P(None,)))\n",
    "\n",
    "@jax.jit\n",
    "def train_step_jit(w, b, x, y):\n",
    "    loss, (dw, db) = jax.value_and_grad(loss_fn, argnums=(0, 1))(w, b, x, y)\n",
    "    w = w - lr * dw\n",
    "    b = b - lr * db\n",
    "    return w, b, loss\n",
    "\n",
    "for step in range(5):\n",
    "    w_jt, b_jt, loss_val = train_step_jit(w_jt, b_jt, x_s, y_s)\n",
    "    print(f\"  Step {step}: loss={loss_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e51b99",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8ea35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Custom Communication: Ring AllReduce (shard_map only) ----\n",
    "# shard_map shines when you need communication patterns JIT can't infer.\n",
    "\n",
    "print(\"\\n=== 5. Custom Communication — Ring Permute (shard_map only) ===\")\n",
    "\n",
    "# ppermute: each device sends its shard to the next device in a ring\n",
    "@shard_map(mesh, in_specs=(P('dp',),), out_specs=P('dp',))\n",
    "def ring_shift(x_shard):\n",
    "    # Shift each shard one device to the right (ring topology)\n",
    "    n = jax.lax.axis_size('dp')\n",
    "    perm = [(i, (i + 1) % n) for i in range(n)]\n",
    "    return jax.lax.ppermute(x_shard, axis_name='dp', perm=perm)\n",
    "\n",
    "x = jnp.arange(8.0)\n",
    "print(f\"Before ring shift: {x}\")\n",
    "print(f\"After  ring shift: {ring_shift(x)}\")\n",
    "# Device 0 gets value from device 7, device 1 from device 0, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c9ec5b",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0996879",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. 2D Mesh: Data + Model Parallelism ----\n",
    "# shard_map can specify collectives on specific mesh axes.\n",
    "\n",
    "print(\"\\n=== 6. 2D Mesh with shard_map ===\")\n",
    "\n",
    "mesh_2d = Mesh(np.array(devices).reshape(4, 2), axis_names=('dp', 'mp'))\n",
    "\n",
    "x = jax.random.normal(key, (4, 8))  # batch=4, features=8\n",
    "w = jax.random.normal(key, (8, 6))  # features=8, outputs=6\n",
    "\n",
    "# --- shard_map: shard data on dp, weights on mp ---\n",
    "@shard_map(\n",
    "    mesh_2d,\n",
    "    in_specs=(P('dp', None), P(None, 'mp')),\n",
    "    out_specs=P('dp', 'mp'),\n",
    ")\n",
    "def matmul_2d_shard(x_shard, w_shard):\n",
    "    # x_shard: (1, 8) per dp device, w_shard: (8, 3) per mp device\n",
    "    return x_shard @ w_shard\n",
    "\n",
    "result_shard = matmul_2d_shard(x, w)\n",
    "print(f\"shard_map 2D result shape: {result_shard.shape}\")\n",
    "\n",
    "# --- jax.jit: same thing, automatically ---\n",
    "@jax.jit\n",
    "def matmul_2d_jit(x, w):\n",
    "    return x @ w\n",
    "\n",
    "x_s = jax.device_put(x, NamedSharding(mesh_2d, P('dp', None)))\n",
    "w_s = jax.device_put(w, NamedSharding(mesh_2d, P(None, 'mp')))\n",
    "result_jit = matmul_2d_jit(x_s, w_s)\n",
    "print(f\"jax.jit  2D result shape: {result_jit.shape}\")\n",
    "print(f\"Results match: {jnp.allclose(result_shard, result_jit, atol=1e-5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11b37c4",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdff41f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. Summary: When to Use Which ----\n",
    "\n",
    "print(\"\"\"\n",
    "=== When to Use shard_map vs jax.jit ===\n",
    "\n",
    "Use jax.jit + NamedSharding (recommended default):\n",
    "  - Standard data/model/FSDP parallelism\n",
    "  - Clean, high-level code (write global-view, JIT handles communication)\n",
    "  - Less error-prone (no manual collectives)\n",
    "  - The XLA compiler optimizes communication automatically\n",
    "\n",
    "Use shard_map when you need:\n",
    "  - Custom collective patterns (ring allreduce, pipeline stages, ppermute)\n",
    "  - Fine-grained control over what each device does\n",
    "  - Debugging: see exactly what runs on each shard\n",
    "  - Non-standard communication that JIT can't infer\n",
    "  - Expert-level optimization of communication overlap\n",
    "\n",
    "Both use:\n",
    "  - Mesh for device layout\n",
    "  - PartitionSpec for describing how arrays are sharded\n",
    "\n",
    "Key difference:\n",
    "  shard_map: you write LOCAL code (per-shard) + explicit collectives\n",
    "  jax.jit:   you write GLOBAL code (full-array) + automatic sharding\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
